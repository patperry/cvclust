\documentclass[11pt]{article}
\usepackage{epsf}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage[margin=2.5cm]{geometry}
\usepackage{amsmath}
\usepackage{footnote}
\usepackage[bottom]{footmisc}
\usepackage[singlelinecheck=false]{caption}
\usepackage{caption}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{graphics}
\usepackage{float}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{lscape}
\usepackage{array}
\textheight=8.7in
\usepackage{setspace}
\usepackage{verbatim}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{longtable}

\def\threedigits#1{%
  \ifnum#1<100 0\fi
  \ifnum#1<10 0\fi
  \number#1}

\topmargin=0.1in \oddsidemargin=-0.1cm \evensidemargin=-0.1cm

\paperheight=11in \paperwidth=8.5in \marginparwidth=0in

\marginparsep=0in \textwidth=6.5in \headheight=0in \headsep=0in

\onehalfspacing
\def\argmax{\mathop{\rm arg\,max}}
%\usepackage{xspace,epsfig,subfig}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newenvironment{sketch}{\noindent\emph{Proof Sketch:}}{$\quad \Box$}
%\newenvironment{proof}{\noindent\emph{Proof:}}{$\quad \Box$}

\bibpunct{(}{)}{;}{a}{,}{,}

% operators
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\CV}{\operatorname{CV}}
\newcommand{\PE}{\operatorname{PE}}
\newcommand{\E}{\operatorname{E}}
\DeclareMathOperator*{\cov}{cov}
\DeclareMathOperator*{\tr}{tr}
\DeclareMathOperator*{\var}{var}

% convergence
\newcommand{\toas}{\overset{\mathit{a.s.}}{\to}}

% sets
\newcommand{\R}{\mathbb{R}}
\newcommand{\sA}{\mathcal{A}}
\newcommand{\sbA}{\mathcal{\bar A}}
\newcommand{\sC}{\mathcal{C}}
\newcommand{\sE}{\mathcal{E}}
\newcommand{\sN}{\mathcal{N}}
\newcommand{\sX}{\mathcal{X}}

% scalars
\newcommand{\bpi}{\bar \pi}

% vectors
\newcommand{\muX}{\mu^{X}}
\newcommand{\muY}{\mu^{Y}}
\newcommand{\bmuX}{\bar \mu^{X}}
\newcommand{\bmuY}{\bar \mu^{Y}}

% matrices
\newcommand{\dataX}{\mathfrak{X}}
\newcommand{\SigmaY}{\Sigma^Y}
\newcommand{\Xtrain}{X_{\text{train}}}
\newcommand{\Ytrain}{Y_{\text{train}}}
\newcommand{\Xtest}{X_{\text{test}}}
\newcommand{\Ytest}{Y_{\text{test}}}

% class labels
\newcommand{\hGX}{\hat G^{X}}
\newcommand{\hGY}{\hat G^{Y}}


\begin{document}
\title{Estimating the number of clusters using Cross Validation}
\author{Wei Fu \qquad Patrick O. Perry \\\\ New York University}
\date{}
\maketitle
\begin{abstract}
Many clustering methods, including $k$-means, require the user to specify the
number of clusters as an input parameter. A variety of methods have been
devised to choose the number of clusters automatically, but they often rely on
strong modeling assumptions. We propose a data-driven approach to estimate the
number of clusters based on a novel form of cross-validation. This differs
from ordinary cross-validation, because clustering is fundamentally an
unsupervised learning problem. Simulation and real data analysis results show
that our proposed method outperforms existing methods, especially in
high-dimensional settings with heavy-tailed data.
\end{abstract}


\section{Introduction}

As a main task of exploratory data analysis, clustering organizes unlabeled
observations into groups such that observations in same group are more similar
compare to those in different group. Clustering is an important topic in
unsupervised learning because it can reveal the internal structure of data
through grouping, segment the data through partitioning and summarize data for
other purposes such as dimension reduction. It has being widely used in
various fields such as psychology, biology, statistics and machine learning
including pattern recognition, image segmentation etc.


After being proposed more than $50$ years, $k$-means remains one of the most
popular and widely used clustering algorithms \citep{jain2010data}. Like many
other clustering methods, $k$-means requires an input parameter $k$, the
number of clusters, to be specified by the user. Automatically and
quantitatively deciding such parameter is important and yet unsolved problem
\citep{fujita2014non}. Various methods have been proposed to tackle this
difficulty. One ad hoc approach is to explore the relationship between $W_k$
(within-cluster dispersion) and the number of cluster $k$ for a certain
clustering method such as $k$-means. Since $W_k$ decreases as $k$ increases,
one usually find the ``elbow" of curve obtain by plotting $W_k$ versus $k$ as
the appropriate number of clusters. The example on the top row of
Figure~\ref{fig:elbow} demonstrates such approach for data with $k=4$, where
the ``elbow" point indeed reveals the true number of clusters. This is based
on the idea that under partitioning data set has more impact than over
partitioning data set in terms of $W_k$. However, locating the ``elbow" point
is somewhat subjective and sometimes is not appropriate to select the optimal
$k$. The second example on the bottom row of Figure \ref{fig:elbow} shows a
situation where there is no clear choice of the ``elbow" point -- both $k=2$
and $k=3$ can be viewed as the ``elbow" point. What's more, the true $k=4$ can
never be selected as the optimal $k$ using such approach in this case since it
can hardly be viewed as the ``elbow" of the curve.


\begin{figure}
\centering
\begin{minipage}{\linewidth}
  \begin{minipage}{0.45\linewidth}
    \includegraphics[width=\linewidth]{demo/elbow/correct-data.pdf}
  \end{minipage}
  \hspace{0.05in}
  \begin{minipage}{0.45\linewidth}
    \includegraphics[width=\linewidth]{demo/elbow/correct-withinss.pdf}
  \end{minipage}
\end{minipage}
\begin{minipage}{\linewidth}
  \begin{minipage}{0.45\linewidth}
    \includegraphics[width=\linewidth]{demo/elbow/incorrect-data.pdf}
  \end{minipage}
  \hspace{0.05in}
  \begin{minipage}{0.45\linewidth}
    \includegraphics[width=\linewidth]{demo/elbow/incorrect-withinss.pdf}
  \end{minipage}
\end{minipage}
\caption{Left panels show the $(X,Y)$ data points; right panels
  show the corresponding values of the within-cluster sum of squares $W_k$
plotted against the number of clusters, $k$.}
\label{fig:elbow}
\end{figure}


Recently, there are several new proposals to find the $k$ automatically. Gap
statistics \citep{tibshirani2001estimating} estimates $k$ by comparing the
change in within-cluster dispersion with that expected under an appropriate
reference null distribution. Specifically, the graph of $\log(W_k)$ is
compared with its expectation under an appropriate null reference distribution
of the data. The value of $k$ associated with the largest gap between
$\log(W_k)$ and the reference curve is selected as optimal $k$.
\citet{sugar2003finding} proposed an approach which finds the number of
clusters based on distortion, a quantity that measures the average distance,
per dimension. It's backed by a rigorous theoretical justification based on
information-theoretic ideas.   \citet{fraley2002model}'s Model-based method
employs the EM algorithm to estimate the parameters in Gaussian mixture model,
and select the best model ($k$) using BIC criterion. Stability-based criterion
is also proposed to locate the best $k$ by some authors such as
\citet{ben2001stability}, \citet{wang2010consistent} and
\citet{fang2012selection}. \citet{chiang2010intelligent} provides a nice review
of existing methods for finding the right $k$ in published literature.


Most existing methods are either model based method requires strong parameter
assumptions or ad hoc method utilizes within-cluster dispersion. Although many
view selecting the number of clusters as a model selection problem, very few
approaches this problem from the prediction point of view. Select model with
smallest prediction error via cross-validation is one of the simplest and most
widely used model selection techniques in supervised learning. The lack of
true class (label) in data set makes the adoption of cross-validation into
unsupervised leaning problem difficult. Perhaps the only exception is
\cite{tibshirani2005cluster}, which selects the optimal $k$ by prediction
strength. The strategy is to first cluster the test data and training data
into $k$ clusters respectively. Then, for each pair of observations that are
assigned to the same test cluster, algorithm determines whether they are also
assigned to the same cluster based on the training centers. The intuition here
is, if $k=k_0$, the true number of clusters, then the $k$ training set
clusters will be similar to the $k$ test clusters, and hence will predict them
well. However, a specifically defined prediction error measure is used in such
procedure, which is quite different from the one commonly used in
cross-validation procedure in supervised learning. Therefore the well
understood properties of cross-validation procedure in supervised learning
cannot be carried over to the prediction strength method, makes the
interpretation of its result difficult.


Our proposed method is a complete data-driven approach which doesn't rely on
any parametric assumptions. Through novel form of partitioning data set, we
are able to employing the cross-validation procedure in clustering exactly the
same way as in supervised learning problem. Hence, it's easy for reader to
understand the intuition behind our proposed method. Simulation and real data
application shows the superior performance of our proposed method compare with
existing methods in high-dimension settings and heavy-tailed data. Since the
embedded cross-validation procedure is well understood, it also makes our
method potentially easily to be extended in future study. 
 


\section{Cross-validation for selecting the number of clusters}

Cross-validation is commonly used for model selection in supervised learning
problems.  In these settings, the data comes in the form of $N$
predictor-response pairs, $(X_1, Y_1), \dotsc, (X_N, Y_N)$, with $X_i \in
\R^{p}$ and $Y_i \in \R^{q}$.  The data can be represented as a matrix with
$N$ rows and $p + q$ columns.  We partition the data into $K$ hold-out
``test'' subsets, with $K$ typically chosen to be $5$ or $10$.  For each
``fold'' $r$ in the range $1, \dotsc, K$, we permute the rows of the data
matrix to get $\dataX$, a matrix with the $r$th test subset as its trailing
rows.  We partition $\dataX$ as
\[
  \dataX =
  \begin{bmatrix}
    \Xtrain & \Ytrain \\
    \Xtest  & \Ytest
  \end{bmatrix}.
\]
We use the training rows $[ \Xtrain\ \Ytrain ]$ to fit a regression model
$\hat Y = \hat Y(X)$, and then evaluate the performance of this model on the
test set, computing the cross-validation error $\|\Ytest - \hat Y(\Xtest)\|^2$
or some variant thereof.  We choose the model with the smallest
cross-validation error, averaged over all $K$ folds.


In unsupervised learning problems like factor analysis and clustering, the
features of the observations are not naturally partitioned into ``predictors''
and ``responses'', so we cannot directly apply the cross-validation procedure
described above.  For factor analysis, there are at least two versions of
cross-validation.  \citet{wold78cross} proposed a ``speckled'' holdout, where
in each fold we leave out a subset of the elements of the data matrix.  Wold's
procedure works well empirically, but does not have any theoretical support,
and it requires a factor analysis procedure that can handle missing data.
\citet{owen2009bi} proposed a scheme called ``bi-cross-validation'' wherein
each fold designates a subset of the data matrix columns to be response and a
subset of the rows to be test data.  This generalized a procedure due to
\citet{gabriel2002biblot}, who proposed holding out a single column and a
single row at each fold.  Owen and Perry proved that this procedure is
self-consistent, in the sense that it performs the correct model selection in
the absence of noise, and \citet{perry2009cross} provided more theoretical
support.


In this report, we extend the Wold and Gabriel methods to the clustering
problem, specifically to choose an appropriate number of clusters for a
dataset.  We prove that the Gabriel method is self-consistent, and we analyze
some of its properties in the presence of noise.  We compare these methods to
state-of-the-art algorithms, and show that both are competitive.



%% [POP] This paragraph doesn't fit into the section.  Maybe put it in the introduction?
%% 
%% Cross-Validation technique, one of the most commonly used and popular model
%% selecting method in supervised learning, can not be used naively in
%% unsupervised learning context. Since prediction error of new observation is
%% calculated by its distance to the nearest cluster center, more cluster centers
%% means much tighter fit to the feature space, and hence smaller distance
%% (prediction error) of observation to its nearest cluster center. This holds
%% even when the prediction is evaluated on an independent test set
%% \citep{hastie2009elements}. Therefore, CV prefers larger $k$ if we do the CV
%% naively.




We now give the details of how to implement
the Gabriel cross-validation to locate the optimal cluster number $k$. The
Wold cross-validation algorithm is described in Appendix $A$.


\subsection{Gabriel CV algorithm}
\label{sec:gabriel-cv-algorithm}

We are given a data matrix with $N$ rows and $P$ columns.  In each fold of
cross-validation, we permute the rows and columns of the data matrix and then
partition the rows and columns as $N = n + m$ and $P = p + q$ for 
non-negative integers $n$, $m$, $p$, and $q$.  We treat the first $p$
columns as ``predictors'' and the last $q$ columns as ``responses'';
similarly, we treat the first $n$ rows as ``training'' and the last $m$ rows
as ``test''.  In block form, the permuted data matrix is
\[
  \dataX
  =
  \begin{bmatrix}
    \Xtrain & \Ytrain \\
    \Xtest  & \Ytest
  \end{bmatrix},
\]
where
$\Xtrain \in \R^{n \times p}$,
$\Ytrain \in \R^{n \times q}$,
$\Xtest \in  \R^{m \times p}$,
and
$\Ytest \in  \R^{m \times q}$.


Given such a partition of $\dataX$, we perform four steps for each value of
$k$, the number of clusters:
\begin{enumerate}
  \item \label{step:gabriel-cluster}
    \textbf{Cluster:}
    Cluster $Y_{1}, \dotsc, Y_n$, the rows of $\Ytrain$, yielding the
    assignment rule $\hGY : \R^q \to \{ 1, \dotsc, k \}$ and the
    cluster means $\bmuY_1, \dotsc, \bmuY_k$.  Set $\hGY_i = \hGY(Y_i)$ to
    be the assigned cluster for row $i$.

  \item \label{step:gabriel-classify}
    \textbf{Classify:}
    Take $X_{1}, \dotsc, X_n$, the rows of $\Xtrain$ to be predictors,
    and take $\hGY_1, \dotsc, \hGY_n$ to be corresponding class labels.  Use
    the pairs $\{ (X_i, \hGY_i) \}_{i=1}^{n}$ to train a classifier
    $\hGX : \R^p \to \{ 1, \dotsc, k \}$.

  \item \label{step:gabriel-predict}
    \textbf{Predict:}
    Apply the classifier to $X_{n+1}, \dotsc, X_{n+m}$, the rows of
    $\Xtest$, yielding predicted classes $\hGX_i = \hGX(X_i)$ for
    $i = n+1, \dotsc, n+m$.  For each value of $i$ in this range, compute
    predicted response $\hat Y_i = \bmuY(\hGX_i)$, where
    $\bmuY(g) = \bmuY_g$.

  \item \label{step:gabriel-evaluate}
    \textbf{Evaluate:}
    Compute the cross-validation error
    \[
      \CV(k) = \frac{1}{m} \sum_{i=n+1}^{n+m} \|Y_i - \hat Y_i\|^2,
    \]
    where $Y_{n+1}, \dotsc, Y_{n+m}$ are the rows of $\Ytest$.
\end{enumerate}
\noindent
In principle, we could use any clustering and classification methods in
steps~\ref{step:gabriel-cluster} and~\ref{step:gabriel-classify}.  In this
report, we use $k$-means as the clustering algorithm.  For the classification
step, we compute the mean value of $X$ for each class; we assign an
observation to class $g$ if that class has the closest mean (randomly breaking
ties between classes).  The classification step is equivalent to linear
discriminant analysis with equal class priors and identity noise covariance
matrix.


To choose the folds, we randomly partition the rows and columns into $K$ and
$L$ subsets, respectively.  Each fold is indexed by a pair $(r,s)$ of
integers, with $r \in \{1, \dotsc, K\}$ and $s \in \{1, \dotsc, L\}$.  Fold
$(r,s)$ treats the $r$th row subset as ``test'', and the $s$th column subset
as ``response''.  We typically take $K = 5$ and $L = 2$.  For the number of
clusters, we select the value of $k$ that minimizes the average of $\CV(k)$
over all $K \times L$ folds (choosing the smallest value of $k$ in the event
of a tie).



\section{Self-Consistency of Gabriel CV method}


This section gives the self-consistency proof of the proposed Gabriel method.
Specifically, we will show that under appropriate conditions, in the absence
of noise, the Gabriel cross-validation procedure finds the optimal number of
clusters.


Because $k$-means algorithm is essential to the method, we review the
procedure here.  Given a set of observations $\{ x_1, \dotsc ,x_n \}$, and a
specified the number of clusters $k$, the goal of the $k$-means procedure is
to find a set of $k$ or cluster centers $A = \{ a_1, \dotsc, a_k \} $
minimizing the within cluster dispersion
\[
  W(A) = \sum_{i=1}^{n} \min_{a \in A} \|x_i - a\|^2.
\]
This implicitly defines a cluster assignment rule
\[
  g(x) = \argmin_{g \in \{1, \dotsc, k\}} \|x - a_g\|^2,
\]
with ties broken arbitrarily.  We will assume that the $k$-means procedure
finds an optimal solution, $A$, but we will not assume that this solution is
unique.


It will suffice to analyze a single fold of the cross-validation procedure.
As in in section~\ref{sec:gabriel-cv-algorithm} we assume that the $P$
variables of the data set have been partitioned into $p$ predictor variables
represented in vector~$X$ and $q$ response variables represented in
vector~$Y$.  The $N$ observations have been divided into two sets: $n$ train
observations and $m$ test observations.  The following theorem gives
conditions for Gabriel CV to recover the true number of clusters in the
absence of noise.


\begin{theorem}\label{thm:self-consistency}

Let $\{ (X_i, Y_i) \}_{i=1}^{n+m}$ be the data from a single fold of Gabriel
cross-validation.  For any $k$, let $CV(k)$ be the cross-validation error for
this fold, computed as described in Section~\ref{sec:gabriel-cv-algorithm}.
We will assume that there are $K$ true centers $\mu(1), \dotsc,\mu(K)$, with
the $g$th cluster center partitioned as $\mu(g) = \bigl(\muX(g),
\muY(g)\bigr)$ for $g = 1, \dotsc, K$.  Suppose that
\begin{enumerate}[label=(\roman*)]
  \item \label{asn:self-consistency-noiseless}
    Each observation $i$ has a true cluster $G_i \in \{ 1, \dotsc, K \}$.
    There is no noise, so that $X_i = \muX({G_i})$ and $Y_i = \muY(G_i)$ for
    $i = 1, \dotsc, n+m$.

  \item \label{asn:self-consistency-distinct-mux}
    The vectors $\muX(1), \dotsc,\muX(K)$ are all distinct.

  \item \label{asn:self-consistency-distinct-muy}
    The vectors $\muY(1), \dotsc,\muY(K)$ are all distinct.

  \item \label{asn:self-consistency-train}
    The training set contains at least one member of each cluster: for all $g$
    in the range $1, \dotsc, K$, there exists at least one $i$ in the range
    $1, \dotsc, n$ such that $G_i = g$.

  \item \label{asn:self-consistency-test}
    The test set contains at least one member of each cluster: for all $g$ in
    the range $1, \dotsc, K$, there exists at least one $i$ in the range $n+1,
    \dotsc, n+m$ such that $G_i = g$.

\end{enumerate}
Then $CV(k) < CV(K)$ for $k < K$, and $CV(k) = CV(K)$ for $k > K$, so that
Gabriel CV correctly chooses $k = K$.
\end{theorem}

This theorem is implied by the following two lemmas.

\begin{lemma}
Suppose that the assumptions of Theorem~\ref{thm:self-consistency} are in
force.  If $k < K$, then $\CV(k) > 0$.
\end{lemma}
\begin{proof}
By definition,
\[
  \CV(k)
    =
      \sum_{i=n+1}^{n+m}
        \| Y_i - \bmuY (\hGX_i) \|^2,
\]
where $\bmuY(g)$ is the center of cluster $g$ returned from applying $k$-means
to $Y_1, \dotsc, Y_n$.  Assumptions~\ref{asn:self-consistency-noiseless}
and~\ref{asn:self-consistency-test}, imply that as $i$ ranges over the test
set $n+1, \dotsc, n+m$, the response $Y_i$ ranges over all distinct values in
$\{ \muY(1), \dotsc, \muY(K) \}$.
Assumption~\ref{asn:self-consistency-distinct-muy} implies that there are
exactly $K$ such distinct values.  However, there are only $k$ distinct values
of $\bmuY(g)$.  Thus, at least one summand
\(
  \| Y_i - \bmuY(\hGX_i) \|^2
\)
is nonzero.  Therefore,
\(
  \CV(k) > 0.
\)
\end{proof}


\begin{lemma}
Suppose that the assumptions of Theorem~\ref{thm:self-consistency} are in
force.  If $k \geq K$, then $\CV(k) = 0$.
\end{lemma}
\begin{proof}
From assumptions~\ref{asn:self-consistency-noiseless},
\ref{asn:self-consistency-distinct-muy},
and~\ref{asn:self-consistency-train}, we know the cluster centers
gotten from applying $k$-means to $Y_1, \dotsc, Y_n$ must include
$\muY(1), \dotsc, \muY(K)$.  Without loss of generality, suppose that
$\bmuY(g) = \muY(g)$ for $g = 1, \dotsc, K$.  This implies that
$\hGY_i = G_i$ for $i = 1, \dotsc, n$.  Thus, employing
assumption~\ref{asn:self-consistency-noiseless} again, we get that
$\bmuX(g) = \muX(g)$ for $g = 1, \dotsc, K$.


Since assumption~\ref{asn:self-consistency-distinct-mux} ensures that
$\muX(1), \dotsc, \muX(K)$ are all distinct, we must have that $\hGX_i = G_i$
for all $i = 1, \dotsc, m+n$.  In particular, this implies that $\bmuY(\hGX_i)
= Y_i$ for $i = 1, \dotsc, m+n$, so that $\CV(k) = 0$.
\end{proof}



\section{Asymptotic Analysis of $k$-means}

To analyze the asymptotic properties of of Gabriel cross validation, we first
need to understand the properties of the $k$-means procedure.  We will use
results from \citet{pollard1981strong}, who analyzed $k$-means and showed that
the within-cluster sum of squares converges to a population quantity.  To
introduce Pollard's result, for any finite set $A \subseteq \R^d$ and
probability measure $P$ supported on $\R^d$, define the population
within-cluster sum of squares as
\[
  W(A, P) = \int \min_{a \in A} \|x - a\|^2 P(dx).
\]
Define
\(
  \sA_k
    =
    \{ A \subseteq \R^d : \text{$A$ contains $k$ or fewer points} \}
\)
and set
\(
  m_k(P) = \inf_{A \in \sA_k}  W(A, P).
\)
Suppose that $X_1, \dotsc, X_n$ are independent draws from $P$, and that $P_n$
is the empirical measure formed by placing mass $1/n$ on each $X_i$.  Let
$A_n$ be a set minimizing $W(A_n, P_n)$.  Pollard gave conditions for $A_n$ to
converge to a minimizer of $W(\cdot, P)$.  His main theorem assumes that the
minimizer of $W(\cdot, P)$ is unique, which is too strong for our purposes
(this fails, for example, with a bivariate normal distribution with identity
covariance whenever $k \geq 2$).  It is straightforward to generalize his
result to cases with multiple maximizers.


To state the generalized version of Pollard's consistency theorem, define
\[
  \sbA_k = \{ \bar A \subseteq \R^d
            : W(\bar A, P) = m_k(P)
              \text{ and $\bar A$ contains $k$ or fewer points} \}.
\]
We will later show that $\sbA_k$ is nonempty and compact whenever $\int
\|x\|^2 P(dx) < \infty$.  Let $H(\cdot, \cdot)$ denote the Hausdorff metric,
defined for compact subsets $A, B$ of $\R^d$ by $H(A,B) < \delta$ if
and only if every point of $A$ is within Euclidean distance $\delta$ of at
least one point of $B$, and vice versa.  We can now state the generalized
$k$-means consistency theorem.


\begin{theorem}[$k$-Means Consistency]\label{thm:kmeans-consistency}
Suppose that $\int \|x\|^2 P(dx) < \infty$ and that $A_n$ minimizes $W(\cdot,
P_n)$ over $\sA_k$.  Then for all $n$ there exists an $\bar A_n \in \sbA_k$
such that $H(A_n, \bar A_n) \to 0\text{ a.s.}$, and $W(A_n, P_n) \to
m_k(P)\text{ a.s.}$ \end{theorem}


The proof of this theorem relies on a series of lemmas, which are extensions
of results given in Pollard's original article.  First, we show that the map
$A \mapsto W(A, P)$ is continuous.  Next, we show that $\sbA_k$ is nonempty
and compact.  Then, it follows that for $n$ large enough, all minimizers of
$W(\cdot, A_n)$ are contained in a compact set $\sE_k$.  The final ingredient
of the proof is Pollard's uniform strong law of large numbers, which says that
$\sup_{A \in \sE_k} |W(A,P) - W(A,P_n)| \to 0\text{ a.s.}$


\begin{lemma}[Continuity]\label{lem:kmeans-continuous}
If $\int \|x\|^2 P(x) < \infty$, then the map $A \mapsto W(A, P)$ is
continuous on $\sA_k$ with respect to the topology induced by the Hausdoff
metric.
\end{lemma}
\begin{proof}
This is a slight extension of the argument given at the end of Pollard's
Section~4.  If $A, B \in \sA_k$ and $H(A, B) < \delta$, then for each $b \in
B$ there exists an $a(b) \in A$ such that $\|b - a(b)\| < \delta$.  Then
\begin{align*}
  W(A, P) - W(B, P)
    &= \int \min_{a \in A} \|x - a\|^2 - \min_{b \in B} \|x - b\|^2 P(dx) \\
    &\leq \int \max_{b \in B} \{ \|x - a(b)\|^2 - \|x - b\|^2 \} P(dx) \\
    &\leq \int \sum_{b \in B} \{ (\|x - b\| + \delta)^2 - \|x - b\|^2 \} P(dx) \\
    &\leq \int \sum_{b \in B}  ( 2 \delta \|x - b\| + \delta^2) P(dx) \\
    &\leq 2 \delta \{M^{1/2} + k \max_{b \in B} \|b\|\} + k \delta^2,
\end{align*}
where $M = \int \|x\|^2 P(dx)$.  Thus,
\[
  |W(A, P) - W(B, P)|
    \leq 2 \delta \{M^{1/2} + k \max_{a \in A} \|a\|\} + 3 k \delta^2.
\]
Hence if $A \in \sA_k$ is any fixed set, then for every $\varepsilon > 0$, we
can find a $\delta$ small enough so that if $B \in \sA_k$ and $H(A, B) <
\delta$, then $|W(A,P) - W(B,P)| < \epsilon$.  Thus, $W(\cdot, P)$ is
continuous at $A$.
\end{proof}


\begin{lemma}[Existence]\label{lem:kmeans-existence}
If $\int \|x\|^2 P(dx) < \infty$ and the support of $P$ contains at least $k$
points, then
\begin{enumerate}
  \item $m_1(P) > m_2(P) > \dotsb > m_k(P)$;
  \item $W(\cdot, P)$ achieves its infimum over $\sA_j$ for $j = 1, \dotsc,
    k$;
  \item the class of sets $\sbA_j = \{ A \in \sA_j : W(A,P) = m_{j}(P) \}$
    is compact (with respect to the Hausdorff metric on $\sA_j$) for $j = 1,
    \dotsc, k$.
\end{enumerate}
\end{lemma}
\begin{proof}
This argument is adapted from the proof in Section~3 of Pollard's paper.  We
first find an $M$ large enough to ensure that every minimizer of $W(\cdot, P)$
must have at least one point in $B(M)$.  Next, we impose additional
constraints on $M$ to ensure that all minimizers of $W(\cdot, P)$ are
contained in $B(5M)$.


Pick~$r$ so that the ball~$K$ of radius~$r$ centered at the origin has
positive $P$~measure.  Take $M$ to be large enough such that
\(
  (M - r)^2 P(K) > \int \|x\|^2 P(dx).
\)
Let $\sC_k$ denote the class of sets $A$ with at most $k$ points and at least
one point in in $B(M)$, the closed ball with radius $M$ centered at the
origin.  Let $A_0$ be the singleton set containing the origin.  If no point of
$A$ is in $B(M)$, then
\(
  W(A, P) \geq (M - r)^2 P(K) > W(A_0, P).
\)
Put concisely, if $A \in \sA_k \setminus \sC_k$, then
\(
  W(A, P) > W(A_0, P) \geq m_k(P).
\)


Next, we proceed by induction on $k$.  If $k = 1$, then there is a unique
minimizer of $W(\cdot, P)$ over $\sA_k$, the set containing the mean $\int x
P(dx)$.  For $k > 1$, take as given that $m_{k-1}(P) = W(\bar A_{k-1}, P)$ for
some set $\bar A_{k-1} \in \sE_{k-1}$.  We will show that there is an $M$
large enough so that $W(\cdot, P)$ attains its infimum over $\sA_k$ and all
minimizers lie in $\sE_k$.  Since $A \mapsto W(\cdot, P)$ is continuous and
$\sE_k$ is compact, the class $\sbA_k$, the preimage of $\{ m_k(P) \}$ under
this map, is compact.


We first show that if $M$ is large enough, then there exists a set $B \in
\sE_k$ and a positive $\varepsilon$ with $W(B,P) < m_{k-1}(P) - \varepsilon$.
This will establish that $m_{k}(P) < m_{k-1}(P)$, and the set $B$ will have
further utility later in the proof.  Take $\bar A_{k-1} \in \sE_{k-1}$ to be a
set with at most $k-1$ points satisfying $m_{k-1}(P) = W(\bar A_{k-1},P)$.
Since the support of $P$ contains least $k$ points, there exists a point~$b_1$
not in $\bar A_{k-1}$ that lies in the support of $P$.  Ensure that $M$ is
large enough so that $b_1 \in B(5M)$.  Take $B \in \sE_k$ to be the set of $k$
points gotten by adjoining $b_1$ to $\bar A_{k-1}$.  Let
\(
  \delta = \min_{a \in \bar A_{k-1}} \| b_1 - a \|
\)
and set $R = \{ x \in \R^d : \|x - b_1\| < \delta/3 \}$; since $b_1$
is in the support of $P$, the neighborhood $R$ has positive $P$~measure.
Further,
\[
  \int_{R} \min_{a \in \bar A_{k-1}} \|x - a\|^2 P(dx)
    \geq
      \int_{R} \min_{a \in \bar A_{k-1}} (\|b_1 - a\| - \delta/3)^2 P(dx)
    =
      (4/9) \delta^2 P(R),
\]
and
\[
  \int_{R} \min_{b \in B} \|x - b\|^2 P(dx)
    =
      \int_{R} \|x - b_1\|^2 P(dx)
    \leq
      (1/9) \delta^2 P(R).
\]
Thus, if
\(
  \varepsilon < (3/9) \delta^2 P(R)
\)
then
\(
  W(B,P)
    \leq
      W(\bar A_{k-1},P) - (3/9) \delta^2 P(R)
    <
      m_{k-1}(P) - \varepsilon.
\)


Now, ensure that $M$ large enough so that
\[
  4 \int_{\|x\| \geq 2M} \|x\|^2 P(dx) < \varepsilon.
\]
We will now show that if $A \in \sC_k \setminus \sE_k$, then $W(A,P) > W(B,P)
\geq m_{k}(P)$, which ensures that $W(\cdot, P)$ does not attain its infimum
over $\sA_k$ outside of $\sE_k$.  To see that this holds, suppose that $A$ is
any set $\sC_k \setminus \sE_k$.  This set has at least one point, $a_1$, in
$B(M)$, and at least one point outside $B(5M)$.  Consider the set $A^\ast$
formed by removing all points of $A$ outside $B(5M)$.  Any point $x$ closer to
a cluster center outside $B(5M)$ than to $a_1$ must satisfy $\|x\| \geq 2M$.
Thus,
\begin{align*}
  W(A^\ast, P)
    &\leq W(A,P) + \int_{\|x\| \geq 2M} \|x - a_1\|^2 P(dx) \\
    &\leq W(A,P) + \int_{\|x\| \geq 2M} (\|x\| + \|a_1\|)^2 P(dx) \\
    &\leq W(A,P) + 4 \int_{\|x\| \geq 2M} \|x\|^2 P(dx) \\
    &< W(A,P) + \varepsilon.
\end{align*}
Since $A^\ast$ has $k-1$ or fewer points, it must satisfy $W(A^\ast, P) \geq
m_{k-1}(P)$, so that $W(A, P) > m_{k-1}(P) - \varepsilon > W(B, P)$.


We have now established that $m_k(P) = \inf \{ W(A,P) : A \in \sE_k \}$, and
that $W(\cdot, P)$ does not obtain its infimum over $\sA_k$ outside of
$\sE_k$.  The final step of the proof is to show $W(\cdot, P)$ obtains its
infimum over $\sE_k$.  This follows from the extreme value theorem, since
$\sE_k$ is compact and, by Lemma~\ref{lem:kmeans-continuous}, the map $A
\mapsto W(A,P)$ is continuous over $\sA_k$.
\end{proof}




\begin{lemma}
\label{lem:pollard-compact}
Suppose that $\int \|x\|^2 P(dx) < \infty$, that the support of $P$ has at
least $k$ points, and that $k > 1$.  Let $A_n$ minimize $W(\cdot, P_n)$ over
$\sA_k$, and that $B_n$ minimize $W(\cdot P_n)$ over $\sA_{k-1}$.  If $W(B_n,
P) \to m_{k-1}(P)$ almost surely as $n \to \infty$, then there exists a finite
$M$ such that as if $n$ is large enough, then, almost surely, $A_n$ is in the
class of sets
\(
  \sE_k = \{ A \subseteq B(5M) : \text{$A$ contains $k$ or fewer points} \},
\)
where $B(5M)$ denotes the Euclidean ball of radius $5M$ centered at the
origin.
\end{lemma}
\begin{proof}
Pollard proves this result, in Section~3 of his paper,  assuming that there is
a unique set of $j$ points minimizing $W(\cdot, P)$ for each value of $j = 1,
\dotsc, k$.  His proof, which is very similar to the proof of
Lemma~\ref{lem:kmeans-existence}, does not use this assumption directly, only
its implication that $m_{k}(P) < m_{k-1}(P)$.  Since we have established his
result in Lemma~\ref{lem:kmeans-existence}, the remainder of his proof applies
without further modification.
\end{proof}


\begin{lemma}[Uniform SLLN]\label{lem:kmeans-uniform-slln}
If $\int \|x\|^2 P(x) < \infty$, then
\(
  \sup_{A \in \sE_k} |W(A, P) - W(A,P_n)| \to 0\text{ a.s.}
\)
\end{lemma}
\begin{proof}
Pollard proves this result at the beginning of his Section~4.
\end{proof}

With Lemmas~\ref{lem:kmeans-continuous}--\ref{lem:kmeans-uniform-slln}, we can
proceed to the proof of Theorem~\ref{thm:kmeans-consistency}.

\begin{proof}[Proof of Theorem~\ref{thm:kmeans-consistency}]
The proof proceeds inductively over $k$.  For $k=1$, the result follows from
the strong law of large numbers.  For $k > 1$, assume that the result holds
for smaller values of $k$.  If the support of $P$ has fewer than $k$ points,
then the result is trivially true.  Otherwise, the induction hypothesis
implies that there exists a $B_n \in \sA_{k-1}$ with $W(B_n, P_n) \to
m_{k-1}(P)\text{ a.s.}$  Hence, Lemma~\ref{lem:pollard-compact} ensures that
there is a finite $M$ such that $A_n \in \sE_k$ almost surely for large
enough~$n$.  Take $M$ to be large enough so that $\sE_k$ also contains
$\sbA_k$.


The set $\sE_k$ is compact and the map $f: \sE_k \to \R$ with $f(A) =
W(A, P)$ is continuous.  Thus, for every $\varepsilon > 0$ there exists an
$\eta > 0$ depending on $\varepsilon$ such that if $A \in \sE_k$ and $H(A,
\bar A) \geq \varepsilon$ for every $\bar A \in \sbA_k$, then $W(A,P) \geq
m_{k}(P) + \eta$.  Let $\bar A_0$ be any member of $\sbA_k$.  By construction,
$W(A_n, P_n) \leq W(\bar A_0, P_n)$.  Lemma~\ref{lem:kmeans-uniform-slln}
ensures that $W(\bar A_0, P_n) \to m_k(P)\text{ a.s.}$  Thus, for $n$ large
enough, $W(A_n, P_n) < m_{k}(P) + \eta$, which implies that there exists an
$\bar A_n \in \sbA_k$ with $H(A_n, \bar A_n) < \varepsilon$.
\end{proof}


The following lemma is not required for the $k$-means consistency theorem, but
it will prove necessary for analyzing the asymptotic behavior of Gabriel
cross-validation.

\begin{lemma}\label{lem:kmeans-separation}
Set $\mu = \int x P(dx)$.  If $\int \|x\|^2 P(dx) < \infty$, then there exists
an $r > 0$ such that for every $A \in \sbA_k$, there is at most one center $a
\in A$ with $\|a - \mu\| \leq r$.
\end{lemma}
\begin{proof}
If the support of $P$ has fewer than $k$ points, then the result is trivial.
Otherwise, Lemma~\ref{lem:kmeans-existence} implies that $m_k(P) <
m_{k-1}(P)$.  Define
\[
  d =
    \inf_{A \in \sA_k}
      \min \{ \|a - a'\| : a,a' \in A, a \neq a' \}.
\]
If $d = 0$, then there exists a sequence $A_n$ in $\sbA_k$ with
\[
  \min_{\substack{a,a' \in A_n \\ a \neq a'}} \|a - a'\| \to 0.
\]
Since $\sbA_k$ is compact, there exists a subsequence $n(m)$ and a set $A_0
\in \sbA_k$ with $A_{n(m)} \to A_0$.  Take $B_n \in \sA_{k-1}$ to be the set
formed by removing one of the centers $a'$ minimizing $\min_{\substack{a \in
A_n \\ a \neq a'}} \|a - a'\|$.  By construction, $H(A_n, B_n) \to 0$, so that
$B_{n(m)} \to A_0$.  Since the map $A \mapsto W(A, P)$ is continuous on
$\sA_k$, we must have $W(B_{n(m)}, P) \to W(A_0, P) = m_k(P)$.  However, this
is impossible since $W(B_n,P) \geq m_{k-1}(P) > m_k(P)$.  It must be the case,
then, that $d > 0$.


Set $r < d/2$, and let $A \in \sbA_k$ be arbitrary.  Take $a_1$ to be the
cluster center in $A$ closest to $\mu$ (breaking ties arbitrarily).  If $\|a_1
- \mu\| > r$, then $\|a - \mu\| > r$ for all centers $a \in A$.  Otherwise, if
$\|a_1 - \mu\| \leq r$ and $a \in A$ is any other cluster center, then
\(
  2r < \|a - a_1\| \leq \|a - \mu\| + \|a_1 - \mu\|
\)
so that
\(
  \|a - \mu\| > r.
\)
\end{proof}



\section{Asymptotic Analysis of Gabriel Cross-Validation}

\subsection{Single Cluster}

Here, we show that Gabriel Cross-Validation is consistent when the data come
from a single cluster; the algorithm picks $k = 1$ with probability tending to
$1$ as the number of rows in the data matrix $\dataX$ increase.


To prove consistency of Gabriel cross-validation, it suffices to analyze a
single fold of the cross-validation procedure.  We will assume the following
setup:


\begin{theorem}
For each $n$, let $m(n)$ be a nonnegative integer with $m(n) \to \infty$ as $n
\to \infty$, and let $\{ (X_{n,i}, Y_{n,i}) \}_{i=1}^{n+m(n)}$ be independent
identically distributed random variables.  For any fixed $k$, let $\CV(k)$
be the cross-validation error from a single Gabriel CV fold, computed as
described in Sec.~\ref{sec:gabriel-cv-algorithm}.  Suppose that
\begin{enumerate}[label=(\roman*)]
\item the distribution for each $(X, Y)$ pair is such that $X$ and $Y$ have
  marginal distributions $P^X$ and $P^Y$, and the vectors $X$ and $Y$ are
  independent of each other;

\item the distribution $P^Y$ satisfies $\int \|y\|^2 P^Y(dy) < \infty$;

\item the support of $P^Y$ contains at least two points.
\end{enumerate}
Then for any fixed $k > 1$, almost surely eventually  $\CV(k) > \CV(1)$.
\end{theorem}
\begin{proof}
Set $\muY = \E(Y)$ and $\SigmaY = \cov(Y)$.  The strong law of large
numbers implies that
\(
  \CV(1) \toas \int \|y - \mu^Y\|^2 P(dy) = \tr(\SigmaY).
\)
If $k > 1$, then Theorem~\ref{thm:kmeans-consistency} and
Lemma~\ref{lem:kmeans-separation} imply that there exists an $r > 0$ such that
almost surely eventually at most one cluster center in
$\{ \bmuY_1, \dotsc, \bmuY_k \}$ is within distance $r$ of $\muY$.  Without
loss of generality suppose that $\bmuY_1$ is the closest cluster center to
$\muY$. [TODO: finish proof]
\end{proof}


\subsection{Well-Separated Clusters with Noise}



\section{Simulation}

In this section, simulation is performed to evaluate the performance of our
proposed methods in locating the ``correct" number of clusters. We compare
with a basket of existing methods including Gap statistics
\citep{tibshirani2001estimating}, Gaussian mixture model-based clustering
\citep{fraley2002model}, CH-index \citep{calinski1974dendrite}, Hartigan
statistics \citep{hartigan1975clustering}, Jump method
\citep{sugar2003finding}, Prediction strength \citep{tibshirani2005cluster},
Bootstrap stability \citep{fang2012selection} in following simulation
settings. We select $L=R$ and $5-$fold cross-validation in row
($m=\frac{4}{5}n$) as default parameter setting for our proposed Gabriel
method. Note that set $L=R$ corresponding to $2-$fold cross-validation in
column.
\begin{enumerate}

  \item Single cluster in $10$ dimensions --- $200$ observations, each
    observation uniformly distributed over $[0,1]$ in each dimension. 

  \item Two clusters in $4$ dimensions --- $50$ i.i.d standard normal
    observations are generated, i.e. generated from multivariate normal
    $\mathcal{N}\left(\boldsymbol\mu,\mathbf{I}\right)$ for each cluster, with
    cluster center $\boldsymbol\mu = (1,0,0,1)$ and $(1,3.5,3.5,1)$
    respectively  

  \item Four clusters in $100$ dimensions ---  Each cluster has $100$ or $150$
    i.i.d standard normal observations, with cluster centers randomly
    generated from multivariate normal distribution
    $\mathcal{N}\left(\mathbf{0},0.65^2\mathbf{I}\right)$

  \item Ten clusters in $100$ dimensions ---  Each cluster has $50$ or $100$
    i.i.d standard normal observations, with cluster centers randomly
    generated from multivariate normal distribution
    $\mathcal{N}\left(\mathbf{0},0.72^2\mathbf{I}\right)$

  \item Four log-normal clusters in $16$ dimensions --- For each cluster, $30$
    or $60$ i.i.d centered log-normal observations are generated from
    $ln\mathcal{N}\left(0,0.5^2 \right)$. The cluster centers are randomly
    generated from multivariate normal distribution
    $\mathcal{N}\left(\mathbf{0},1.2^2\mathbf{I}\right)$

  \item[6.] Three exponential clusters in $20$ dimensions with different
    variance --- $40$ observations in each cluster are generated from centered
    exponential distribution ($exp(\lambda)-1/\lambda$ in each dimension),
    with $\boldsymbol\mu$ randomly generated from
    $\mathcal{N}\left(\mathbf{0},19\mathbf{I}\right)$ and $\lambda = 1, 1/2,
    1/5$ respectively.
\end{enumerate}
Note that in setting 2--6, all clusters are well-separated, i.e. no
overlapping. In fact, any simulated clusters with minimum distance less than
1 unit was discarded, so there is clear definition of true number of
clusters. The parameters in setting 2--6 are chosen such that about half of
the random realization were discarded. The idea is borrowed from
\cite{tibshirani2001estimating}.


Besides the number of clusters selected by each algorithm is returned, we also
give the prediction error (PE) for each algorithm in each setting. The PE is
defined by 
\[
  \PE(k)
    =
      \frac{\|\hat{\mathbf{X}}_k- \boldsymbol{\mu} \|_F}
           {PE_{\text{oracle}}}
\]
where
\[
  \PE_{\text{oracle}} = \min_k \|\hat{\mathbf{X}}_k- \boldsymbol{\mu} \|_F
\]
Matrix $\boldsymbol{\mu}$ consists of true centers of observations in data
matrix $\mathbf{X}$, while $\hat{\mathbf{X}}_k$ is obtained by replacing each
observation in $\mathbf{X}$ by its closest cluster center result from applying
K-means on $\mathbf{X}$ with parameter $k$. Here $k$ is the optimal number of
clusters selected by each algorithm on data $\mathbf{X}$. This is only
possible on simulated data because otherwise one would never know the true
center matrix~$\boldsymbol{\mu}$.


The oracle selected number for each data $\mathbf{X}$ is also given on the
table, which is defined by 
\[
  K_{\text{oracle}}
    =
      \argmin_{k} \|\hat{\mathbf{X}}_k - \boldsymbol{\mu} \|_F
\]

\clearpage

\begin{center}
\footnotesize
\begin{longtable}{lcccccccccccccccc}
\caption{\label{table1} Simulation Results}\\
\toprule
\phantom{Pred.~Strength}
  & \multicolumn{16}{c}{Estimated Number of Clusters} \\
\cmidrule(lr){2-17}
Method
  & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & NA \\
\midrule
\endfirsthead
\multicolumn{17}{c}%
  {\tablename\ \thetable\ -- \textit{Continued from previous page}} \\
\midrule
\phantom{Pred.~Strength}
  & \multicolumn{16}{c}{Estimated Number of Clusters} \\
\cmidrule(lr){2-17}
Method
  & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & NA \\
\midrule
\endhead
\multicolumn{17}{r}{\textit{Continued on next page}} \\
\endfoot
\bottomrule
\multicolumn{17}{l}{\footnotesize
  \textsuperscript{$\ast$} CH, Hartigan, and Stability are excluded from
  Setting~1 because they cannot select $k=1$.
}
\endlastfoot
\input{demo/bench/setting1/results.tex}
\midrule
\input{demo/bench/setting2/results.tex}
\midrule
\input{demo/bench/setting3/results.tex}
\midrule
\input{demo/bench/setting4/results.tex}
\midrule
\input{demo/bench/setting5/results.tex}
\midrule
\input{demo/bench/setting6/results.tex}
\end{longtable}
\end{center}



\noindent
All methods are used with their default parameter setting except for Gap
statistics. We selected the $k$  corresponds to the global maximum of gap
statistics, rather than the default method which selects the smallest $k$ such
that the gap statistics is not more than 1 standard error away from the first
local maximum. This is because among all the possible options, the global
maximum criteria gives the best result in our simulation settings.


The ``\textendash'' sign in Setting~1 is because for CH-index, Hartigen and
Bootstrap stability, the minimum $k$ they can pick is $2$. So they are not
included for comparison since the true $k=1$. Setting $2-4$ have Gaussian
distribution, where the Gaussian mixture model-based clustering suppose to
perform the best. It is only the case for setting $2$. In setting $3$ and $4$,
it has a very poor performance even if the underlying distribution is indeed
Gaussian mixture model. It highlights the difficulties that high dimension
data could cause. Other methods (Gap statistics, CH-index, Hartigan, Jump
method, Prediction strength and Boostrap stability) also have problems in
these two high-dimension settings as we could not find any one with reasonable
performance in terms of finding the correct $k$ in both cases. In terms of PE,
Boostrap stability and Gap statistics have reasonable performance, but come
with the price of picking larger $k$. Our proposed Gabriel method (also Wold
CV method) clearly stands out from the basket of existing methods using either
measure under the high-dimension setting $3$ and $4$.


Setting $5$ and $6$ corresponding to the situation where data are heavy-tailed
instead of normal distributed, coupled with unequal number of observations or
unequal variance for different clusters. From Table \ref{table1}, we can see
that our proposed Gabriel method is robust and has superior performance
compare with other methods. Because one can hardly tell the underlying
distribution of data in practice, the resilience to non-Gaussian data gives
the Gabriel method a clear edge. 



\section{Real data application}

We also applied our proposed method to three real world data sets obtained
from the University of California Irvine machine learning repository. The
first and third data sets are selected because there are clear number of
clusters in those two data sets. The second data set is used as a benchmark
data set since it was widely used in literature.


The first one is congress voting data which consists of voting records of
$98$th United States Congress, $2$nd session. This data set includes votes for
each of the U.S. House of Representatives Congressmen on the $16$ key votes
identified by the \textit{CQA} (Congressional Quarterly Almanac). For each
vote, each Congressman either vote positively ``yea" (voted for/paired
for/announced for),  negatively ``nay'' (voted against/paired
against/announced against) or position unknown ``?". We took out those records
contain ``?" before comparing each algorithm. It results in $232$ remaining
records, with $124$ democrat and $108$ republican. 


The second data set is the well-known Wisconsin breast cancer data set \citep{mangasarian1990pattern}. After excluding the records with missing data, this data set consists records of $683$ patients, each with measurements of nine attributes of their biopsy specimens. It is known that there exist at least two groups of patients: $444$ patients with benign specimens and $239$ patients with malignant specimens.\\

The third data set is the Sonar data returned from two targets -- a metal
cylinder and a rock with similar shape, which is first studied by
\cite{gorman1988analysis} using a neural network. Both targets were impinged
by pulse which was a wide-band linear FM chirp $(ka = 55.6)$. Returns were
collected at a range of $10$ meters and obtained from the cylinder at aspect
angles spanning $90^{\circ}$ and from the rock at aspect angles spanning
$180^{\circ}$. The data set contains $208$ returns ($111$ cylinder returns and
$97$ rock returns), with each composed of $60$ spectral samples, normalized to
take on values between $0$ and $1$. So the data has $60$ features with clearly
$2$ clusters. 


\begin{table}[H]
\begin{center}
\caption{\label{table2} Number of clusters selected by each algorithm}
\begin{tabular}{lccccc}
    \hline                  
 & Congress Voting && Breast Cancer && Sonar \\ \hline                    
CH-index & $2$ && $2$  && $2$    \\
Hartigan & $3$ && $3$  &&  $3$   \\
Jump & $9$ && $9$ && $10$  \\   
Prediction strength & $2$ &&  $2$ && $1$    \\
Bootstrap stability & $2$ &&  $2$ &&  $10$   \\ 
Gap & $10$ && $9$ && $10$  \\   
Gaussian-Mix & $7$ && $5$ && $1$  \\   
Gabriel & $2$  && $3$  && $2$  \\    
Wold & $2$ && $3$ && $10$ \\  \hline 
\end{tabular}
\end{center}
\hspace{0.5in} \footnotesize {All the algorithms executed with their default parameter settings with $k$ ranges from $1$ to $10$}
\end{table} 

Since most congressmen vote base on their parties' interest, $2$ parties
(Democratic and Republican) represent two clusters in this data set. So the
optimal number should be two. Close inspection shows \textit{kmeans} with
$k=2$ separates the two parties very well with the lowest miss-classification
error $10.43\%$ ($k=3$ has $14.78\%$). Note that CH-index and Bootstrap
stability also return $k=2$, but $2$ is the lower bound those methods can
select for $k$. So it's not clear they actually choose $k=2$ or they hit the
lower bound (they would pick $k=1$ if allowed). For the breast cancer data,
it's know to have at least $2$ cluster based on whether it's benign specimens
or not. But it doesn't mean the optimal $k$ should be $2$.
\cite{fujita2014non} noticed that the malign group is quite heterogeneous and
can be further clustered into at least two subgroups. Hence, the result $k=3$
given by our proposed Gabriel method (as well as Wold method) make sense. For
the Sonar data set which is relatively high-dimension, only our proposed
Gabriel method and CH-index selects $k=2$. Majority of the methods collapse to
either select the maximum or select the minimum number, underline how
difficult it was to pick the right $k$ when dimension increases. 


\section{Conclusion}

In this paper, we proposed a novel approach to estimate the number of
clusters. The intuition behind our proposed methods is to transfer the
unsupervised learning problem into supervised learning problem via novel form
of cross validation. We proved the self-consistency for our proposed Gabriel
CV method as well as its asymptotic property with noise, and showed the
robustness of our method by simulation. Our method has very good performance
in our limited simulation settings, and clearly the superior one when data is
high dimension or is heavy-tailed.


\section*{\textbf{APPENDIX}}
\appendix
\section{Wold CV estimation} \label{app:foobar}
\begin{itemize}
	\item For each $k = 1,2,...,k_{max}$
	\begin{enumerate}
		\item Randomly draw some entries in $\mathbf{X}$ missing, keep those hold-out values in vector $V_{true}$
		\item Impute the missing values with column mean or $0$, denote the imputed data as $\mathbf{X}_{new}$
		\item Apply the iterative procedure below until converge or stopping criteria reached
		\begin{itemize}
			\item Apply $K$-mean on data set $\mathbf{X}_{new}$ with parameter $k$
			\item Substitute each observation in $\mathbf{X}_{new}$ by its nearest center, get new data $\mathbf{X}^c_{new}$ ($\mathbf{X}_{new}$ keep the same)
			\item Replace (impute) those imputed values in $\mathbf{X}_{new}$ with the corresponding entries in $\mathbf{X}^c_{new}$
			\item Calculate the difference between the old and newly imputed values, check whether or not they coincide (converge) 
		\end{itemize}
		\item Obtain the last imputed entry values of converged $\mathbf{X}_{new}$, denote it by $V_{converge}$
		\item Calculate the prediction error  $Error_k = ||V_{true} - V_{converge}||^2$ 
	\end{enumerate}
	\item For each CV folder, repeat above procedure and obtain the $Error_k$ for each $k$
	\item Average $Error_k$ across all folders for each $k$, and then select the $k$ corresponding to the minimum average $Error_k$ 
\end{itemize}


\section{Technical Lemmas}

\begin{lemma}\label{lem:truncated-normal-moments}

If $Z$ is a standard normal random variable, then
\[
  \E(Z \mid a < Z < b)
    = - \frac{\varphi(b) - \varphi(a)}
             {\Phi(b) - \Phi(a)}
\]
and
\[
  \E\{(Z - \delta)^2 \mid a < Z < b\}
    = \delta^2 + 1
    - \frac{  (b - 2 \delta) \varphi(b)
            - (a - 2 \delta) \varphi(a)}
           {\Phi(b) - \Phi(a)}
\]
for all constants $a$, $b$, and $\delta$, where $\varphi(z)$ and $\Phi(z)$ are
the standard normal probability density and cumulative distribution functions.
These expressions are valid for $a = -\infty$ or $b = \infty$ by taking
limits.

\end{lemma}
\begin{proof}
We will derive the expression for the second moment.  Integrate to get
\begin{align*}
  \E[ (Z - \delta)^2 1\{Z < b\}]
    &= \int_{-\infty}^b (z - \delta)^2 \varphi(z) \, dz \\
    &= (\delta^2 + 1) \Phi(b) - (b - 2 \delta) \varphi(b).
\end{align*}
Now,
\[
  \E\{(Z - \delta)^2 \mid a < Z < b\}
    =
    \frac{  \E[ (Z - \delta)^2 1\{Z < b\}]
          - \E[ (Z - \delta)^2 1\{Z < a\}]}
         { \Phi(b) - \Phi(a) }.
\]
\end{proof}

Lemma~\ref{lem:truncated-normal-moments} has some important special cases:
\begin{align*}
  \E\{Z \mid Z > 0\} &= 2 \varphi(0) = \sqrt{2 / \pi}, \\
  \E\{(Z - \delta)^2 \mid Z > 0 \}
    &= \delta^2 + 1 - 4 \delta \varphi(0), \\
  \E\{(Z - \delta)^2 \mid Z < 0 \}
    &= \delta^2 + 1 + 4 \delta \varphi(0).
\end{align*}


\section{Counterexample with correlated noise}

Suppose that $(X, Y)$ are jointly mean-zero multivariate normal with unit
marginal variances and correlation $\rho$.  In this case, the data are drawn
from a single cluster; the true number of clusters is~$1$.  We will show that
Gabriel cross-validation is inconsistent if $\rho > 0.5$, in the sense that $k
= 2$ will be preferred to $k = 1$ almost surely for large enough $n$.


Given a sample $(X_1, Y_1), \dotsc, (X_n, Y_n)$, we first apply $k$-means to
$\{ Y_i \}_{i=1}^{n}$.  If $k = 1$ and $n$ is large enough, then the centroid
will be close to $\E(Y) = 0$, in the sense that the centroid converges almost
surely to this value as $n$ tends to infinity \citep{pollard1981strong}.
Similarly, if $k = 2$ and $n$ is large enough, then the centroids will tend to
$\E(Y \mid Y > 0) = \sqrt{2/\pi}$ and $\E(Y \mid Y < 0) = -\sqrt{2/\pi}$.  We
have used Lemma~\ref{lem:truncated-normal-moments} to compute the
expectations.  In $k = 2$ case, then if $n$ is large enough, the cluster
assignment will be determined according to whether $Y > 0$.


In the $k = 1$ case, if $n$ is large enough, the average Gabriel
cross-validation error converges to $\E(Y^2) = 1$. In the $k = 2$ case, if $n$
is large enough and $\rho > 0$, then the linear decision rule for the $X$
variables will be determined according to whether $X > 0$; if $\rho < 0$ then
the decision is according to whether $X < 0$.  In the $\rho > 0$ case, the
error converges to $\E[(Y - a)^2 1\{ X > 0\}] + \E[(Y + a)^2 1\{X < 0\}]$,
where $a = \sqrt{2/\pi}$.  From the joint normality of $X$ and $Y$, it follows
that $Y \mid X$ is normal with mean $\rho X$ and variance $(1 - \rho^2)$, so
that $\E[(Y - a)^2 \mid X] = (\rho X - a)^2 + (1 - \rho^2)$.  Applying
Lemma~\ref{lem:truncated-normal-moments}, in the $k = 2$ case we get that the
Gabriel cross-validation error converges to $1 + a^2 (1 - 2 \rho)$.  In the
$\rho < 0$ case, it converges to $1 + a^2 (1 + 2 \rho)$.  In particular, if
$|\rho| > 0.5$, then the asymptotic cross-validation error for $k = 2$ will be
smaller than for $k = 1$.


We confirm this with a simulation.  We perform $10$ replicates.  In each
replicate, we generate $20000$ observations from a mean-zero bivariate normal
distribution with unit marginal variances and correlation $\rho$.  We perform
a single $2 \times 2$ fold of Gabriel cross-validation and report the
cross-validation mean squared error for the number of clusters $k$ ranging
from $1$ to $5$.  Figure~\ref{fig:nullcorr-equal} shows the cross-validation
errors for all $10$ replicates.  The simulation demonstrates that in the
Gabriel cross-validation criterion chooses the correct answer $k = 1$ whenever
$\rho < 0.5$; the criterion chooses $k = 2$ clusters whenever $|\rho| > 0.5$.

\begin{figure}
\centering
\includegraphics[width=25pc]{demo/nullcorr/equal.pdf}
\caption{Cross-validation error on $10$ replicates, with the number of
clusters $k$ ranging from $1$ to $5$.  Data is generated from two-dimensional
multivariate normal distribution with correlation $\rho$.  The Gabriel
cross-validation criterion chooses the correct answer $k = 1$ whenever
$\rho < 0.5$; the criterion chooses $k = 2$ clusters whenever $|\rho| > 0.5$.}
\label{fig:nullcorr-equal}
\end{figure}


\bibliography{references}
\bibliographystyle{apalike}
\end{document}
