\documentclass[11pt]{article}
\usepackage{epsf}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage[margin=2.5cm]{geometry}
\usepackage{amsmath}
\usepackage{footnote}
\usepackage[bottom]{footmisc}
\usepackage[singlelinecheck=false]{caption}
\usepackage{caption}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{graphics}
\usepackage{float}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{lscape}
\usepackage{array}
\textheight=8.7in
\usepackage{setspace}
\usepackage{verbatim}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{longtable}

\def\threedigits#1{%
  \ifnum#1<100 0\fi
  \ifnum#1<10 0\fi
  \number#1}

\topmargin=0.1in \oddsidemargin=-0.1cm \evensidemargin=-0.1cm

\paperheight=11in \paperwidth=8.5in \marginparwidth=0in

\marginparsep=0in \textwidth=6.5in \headheight=0in \headsep=0in

\onehalfspacing
\def\argmax{\mathop{\rm arg\,max}}
%\usepackage{xspace,epsfig,subfig}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newenvironment{sketch}{\noindent\emph{Proof Sketch:}}{$\quad \Box$}
%\newenvironment{proof}{\noindent\emph{Proof:}}{$\quad \Box$}

\bibpunct{(}{)}{;}{a}{,}{,}

% operators
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\CV}{\operatorname{CV}}
\newcommand{\PE}{\operatorname{PE}}
\newcommand{\E}{\operatorname{E}}
\DeclareMathOperator*{\cov}{cov}
\DeclareMathOperator*{\corr}{corr}
\DeclareMathOperator*{\tr}{tr}
\DeclareMathOperator*{\var}{var}
\newcommand{\T}{T}

% convergence
\newcommand{\toas}{\overset{\mathit{a.s.}}{\to}}

\newcommand{\OhP}{O_p}

% sets
\newcommand{\R}{\mathbb{R}}
\newcommand{\sA}{\mathcal{A}}
\newcommand{\sbA}{\mathcal{\bar A}}
\newcommand{\sC}{\mathcal{C}}
\newcommand{\sE}{\mathcal{E}}
\newcommand{\sN}{\mathcal{N}}
\newcommand{\sX}{\mathcal{X}}

% scalars
\newcommand{\bpi}{\bar \pi}

% vectors
\newcommand{\muX}{\mu^{X}}
\newcommand{\muY}{\mu^{Y}}
\newcommand{\bmuX}{\bar \mu^{X}}
\newcommand{\bmuY}{\bar \mu^{Y}}

% matrices
\newcommand{\dataX}{\mathfrak{X}}
\newcommand{\SigmaY}{\Sigma^Y}
\newcommand{\Xtrain}{X_{\text{train}}}
\newcommand{\Ytrain}{Y_{\text{train}}}
\newcommand{\Xtest}{X_{\text{test}}}
\newcommand{\Ytest}{Y_{\text{test}}}

% class labels
\newcommand{\hGX}{\hat G^{X}}
\newcommand{\hGY}{\hat G^{Y}}


\begin{document}
\title{Estimating the number of clusters using Cross Validation}
\author{Wei Fu \qquad Patrick O. Perry \\\\ New York University}
\date{}
\maketitle
\begin{abstract}
Many clustering methods, including $k$-means, require the user to specify the
number of clusters as an input parameter. A variety of methods have been
devised to choose the number of clusters automatically, but they often rely on
strong modeling assumptions. We propose a data-driven approach to estimate the
number of clusters based on a novel form of cross-validation. This differs
from ordinary cross-validation, because clustering is fundamentally an
unsupervised learning problem. Simulation and real data analysis results show
that our proposed method outperforms existing methods, especially in
high-dimensional settings with heavy-tailed data.
\end{abstract}


\section{Introduction}

As a main task of exploratory data analysis, clustering organizes unlabeled
observations into groups such that observations in same group are more similar
compare to those in different group. Clustering is an important topic in
unsupervised learning because it can reveal the internal structure of data
through grouping, segment the data through partitioning and summarize data for
other purposes such as dimension reduction. It has being widely used in
various fields such as psychology, biology, statistics and machine learning
including pattern recognition, image segmentation etc.


After being proposed more than $50$ years, $k$-means remains one of the most
popular and widely used clustering algorithms \citep{jain2010data}. Like many
other clustering methods, $k$-means requires an input parameter $k$, the
number of clusters, to be specified by the user. Automatically and
quantitatively deciding such parameter is important and yet unsolved problem
\citep{fujita2014non}. Various methods have been proposed to tackle this
difficulty. One ad hoc approach is to explore the relationship between $W_k$
(within-cluster dispersion) and the number of cluster $k$ for a certain
clustering method such as $k$-means. Since $W_k$ decreases as $k$ increases,
one usually find the ``elbow" of curve obtain by plotting $W_k$ versus $k$ as
the appropriate number of clusters. The example on the top row of
Figure~\ref{fig:elbow} demonstrates such approach for data with $k=4$, where
the ``elbow" point indeed reveals the true number of clusters. This is based
on the idea that under partitioning data set has more impact than over
partitioning data set in terms of $W_k$. However, locating the ``elbow" point
is somewhat subjective and sometimes is not appropriate to select the optimal
$k$. The second example on the bottom row of Figure \ref{fig:elbow} shows a
situation where there is no clear choice of the ``elbow" point -- both $k=2$
and $k=3$ can be viewed as the ``elbow" point. What's more, the true $k=4$ can
never be selected as the optimal $k$ using such approach in this case since it
can hardly be viewed as the ``elbow" of the curve.


\begin{figure}
\centering
\begin{minipage}{\linewidth}
  \begin{minipage}{0.45\linewidth}
    \includegraphics[width=\linewidth]{demo/elbow/correct-data.pdf}
  \end{minipage}
  \hspace{0.05in}
  \begin{minipage}{0.45\linewidth}
    \includegraphics[width=\linewidth]{demo/elbow/correct-withinss.pdf}
  \end{minipage}
\end{minipage}
\begin{minipage}{\linewidth}
  \begin{minipage}{0.45\linewidth}
    \includegraphics[width=\linewidth]{demo/elbow/incorrect-data.pdf}
  \end{minipage}
  \hspace{0.05in}
  \begin{minipage}{0.45\linewidth}
    \includegraphics[width=\linewidth]{demo/elbow/incorrect-withinss.pdf}
  \end{minipage}
\end{minipage}
\caption{Left panels show the $(X,Y)$ data points; right panels
  show the corresponding values of the within-cluster sum of squares $W_k$
plotted against the number of clusters, $k$.}
\label{fig:elbow}
\end{figure}


Recently, there are several new proposals to find the $k$ automatically. Gap
statistics \citep{tibshirani2001estimating} estimates $k$ by comparing the
change in within-cluster dispersion with that expected under an appropriate
reference null distribution. Specifically, the graph of $\log(W_k)$ is
compared with its expectation under an appropriate null reference distribution
of the data. The value of $k$ associated with the largest gap between
$\log(W_k)$ and the reference curve is selected as optimal $k$.
\citet{sugar2003finding} proposed an approach which finds the number of
clusters based on distortion, a quantity that measures the average distance,
per dimension. It's backed by a rigorous theoretical justification based on
information-theoretic ideas.   \citet{fraley2002model}'s Model-based method
employs the EM algorithm to estimate the parameters in Gaussian mixture model,
and select the best model ($k$) using BIC criterion. Stability-based criterion
is also proposed to locate the best $k$ by some authors such as
\citet{ben2001stability}, \citet{wang2010consistent} and
\citet{fang2012selection}. \citet{chiang2010intelligent} provides a nice review
of existing methods for finding the right $k$ in published literature.


Most existing methods are either model based method requires strong parameter
assumptions or otherwise lack of clear interpretation and theoretical justification.
Although many view selecting the number of clusters as a model selection problem, very few
approaches this problem from the prediction point of view. Select model with
smallest prediction error via cross-validation is one of the simplest and most
widely used model selection techniques in supervised learning. The lack of
true class (label) in data set makes the adoption of cross-validation into
unsupervised leaning problem difficult. One exception is
\cite{tibshirani2005cluster}, which selects the optimal $k$ by prediction
strength. The strategy is to first cluster the test data and training data
into $k$ clusters respectively. Then, for each pair of observations that are
assigned to the same test cluster, algorithm determines whether they are also
assigned to the same cluster based on the training centers. The intuition here
is, if $k=k_0$, the true number of clusters, then the $k$ training set
clusters will be similar to the $k$ test clusters, and hence will predict them
well. However, a specifically defined prediction error measure is used in such
procedure, which is quite different from the one commonly used in
cross-validation procedure in supervised learning. Therefore the well
understood properties of cross-validation procedure in supervised learning
cannot be carried over to the prediction strength method, makes the
interpretation of its result difficult. \citet{wang2010consistent} also uses 
cross-validation to select the optimal $k$. Instead of selecting $k$ which minimizes 
prediction error, such method picks $k$ which minimizes the specifically defined clustering instability.


Our proposed method is a complete data-driven approach which doesn't rely on
any parametric assumptions. Through novel form of partitioning data set, we
are able to employing the cross-validation procedure in clustering exactly the
same way as in supervised learning problem. Hence, it's easy for reader to
understand the intuition behind our proposed method. Simulation and real data
application shows the superior performance of our proposed method compare with
existing methods in high-dimension settings and heavy-tailed data. Since the
embedded cross-validation procedure is well understood, it also makes our
method potentially easily to be extended in future study. 
 


\section{Cross-validation for selecting the number of clusters}

Cross-validation is commonly used for model selection in supervised learning
problems.  In these settings, the data comes in the form of $N$
predictor-response pairs, $(X_1, Y_1), \dotsc, (X_N, Y_N)$, with $X_i \in
\R^{p}$ and $Y_i \in \R^{q}$.  The data can be represented as a matrix with
$N$ rows and $p + q$ columns.  We partition the data into $K$ hold-out
``test'' subsets, with $K$ typically chosen to be $5$ or $10$.  For each
``fold'' $r$ in the range $1, \dotsc, K$, we permute the rows of the data
matrix to get $\dataX$, a matrix with the $r$th test subset as its trailing
rows.  We partition $\dataX$ as
\[
  \dataX =
  \begin{bmatrix}
    \Xtrain & \Ytrain \\
    \Xtest  & \Ytest
  \end{bmatrix}.
\]
We use the training rows $[ \Xtrain\ \Ytrain ]$ to fit a regression model
$\hat Y = \hat Y(X)$, and then evaluate the performance of this model on the
test set, computing the cross-validation error $\|\Ytest - \hat Y(\Xtest)\|^2$
or some variant thereof.  We choose the model with the smallest
cross-validation error, averaged over all $K$ folds.


In unsupervised learning problems like factor analysis and clustering, the
features of the observations are not naturally partitioned into ``predictors''
and ``responses'', so we cannot directly apply the cross-validation procedure
described above.  For factor analysis, there are at least two versions of
cross-validation.  \citet{wold78cross} proposed a ``speckled'' holdout, where
in each fold we leave out a subset of the elements of the data matrix.  Wold's
procedure works well empirically, but does not have any theoretical support,
and it requires a factor analysis procedure that can handle missing data.
\citet{owen2009bi} proposed a scheme called ``bi-cross-validation'' wherein
each fold designates a subset of the data matrix columns to be response and a
subset of the rows to be test data.  This generalized a procedure due to
\citet{gabriel2002biblot}, who proposed holding out a single column and a
single row at each fold.  Owen and Perry proved that this procedure is
self-consistent, in the sense that it performs the correct model selection in
the absence of noise, and \citet{perry2009cross} provided more theoretical
support.


In this report, we extend the Wold and Gabriel methods to the clustering
problem, specifically to choose an appropriate number of clusters for a
dataset.  We prove that the Gabriel method is self-consistent, and we analyze
some of its properties in the presence of noise.  We compare these methods to
state-of-the-art algorithms, and show that both are competitive.



%% [POP] This paragraph doesn't fit into the section.  Maybe put it in the introduction?
%% 
%% Cross-Validation technique, one of the most commonly used and popular model
%% selecting method in supervised learning, can not be used naively in
%% unsupervised learning context. Since prediction error of new observation is
%% calculated by its distance to the nearest cluster center, more cluster centers
%% means much tighter fit to the feature space, and hence smaller distance
%% (prediction error) of observation to its nearest cluster center. This holds
%% even when the prediction is evaluated on an independent test set
%% \citep{hastie2009elements}. Therefore, CV prefers larger $k$ if we do the CV
%% naively.




We now give the details of how to implement
the Gabriel cross-validation to locate the optimal cluster number $k$. The
Wold cross-validation algorithm is described in Appendix $A$.


\subsection{Gabriel CV algorithm}
\label{sec:gabriel-cv-algorithm}

We are given a data matrix with $N$ rows and $P$ columns.  In each fold of
cross-validation, we permute the rows and columns of the data matrix and then
partition the rows and columns as $N = n + m$ and $P = p + q$ for 
non-negative integers $n$, $m$, $p$, and $q$.  We treat the first $p$
columns as ``predictors'' and the last $q$ columns as ``responses'';
similarly, we treat the first $n$ rows as ``training'' and the last $m$ rows
as ``test''.  In block form, the permuted data matrix is
\[
  \dataX
  =
  \begin{bmatrix}
    \Xtrain & \Ytrain \\
    \Xtest  & \Ytest
  \end{bmatrix},
\]
where
$\Xtrain \in \R^{n \times p}$,
$\Ytrain \in \R^{n \times q}$,
$\Xtest \in  \R^{m \times p}$,
and
$\Ytest \in  \R^{m \times q}$.


Given such a partition of $\dataX$, we perform four steps for each value of
$k$, the number of clusters:
\begin{enumerate}
  \item \label{step:gabriel-cluster}
    \textbf{Cluster:}
    Cluster $Y_{1}, \dotsc, Y_n$, the rows of $\Ytrain$, yielding the
    assignment rule $\hGY : \R^q \to \{ 1, \dotsc, k \}$ and the
    cluster means $\bmuY_1, \dotsc, \bmuY_k$.  Set $\hGY_i = \hGY(Y_i)$ to
    be the assigned cluster for row $i$.

  \item \label{step:gabriel-classify}
    \textbf{Classify:}
    Take $X_{1}, \dotsc, X_n$, the rows of $\Xtrain$ to be predictors,
    and take $\hGY_1, \dotsc, \hGY_n$ to be corresponding class labels.  Use
    the pairs $\{ (X_i, \hGY_i) \}_{i=1}^{n}$ to train a classifier
    $\hGX : \R^p \to \{ 1, \dotsc, k \}$.

  \item \label{step:gabriel-predict}
    \textbf{Predict:}
    Apply the classifier to $X_{n+1}, \dotsc, X_{n+m}$, the rows of
    $\Xtest$, yielding predicted classes $\hGX_i = \hGX(X_i)$ for
    $i = n+1, \dotsc, n+m$.  For each value of $i$ in this range, compute
    predicted response $\hat Y_i = \bmuY(\hGX_i)$, where
    $\bmuY(g) = \bmuY_g$.

  \item \label{step:gabriel-evaluate}
    \textbf{Evaluate:}
    Compute the cross-validation error
    \[
      \CV(k) = \frac{1}{m} \sum_{i=n+1}^{n+m} \|Y_i - \hat Y_i\|^2,
    \]
    where $Y_{n+1}, \dotsc, Y_{n+m}$ are the rows of $\Ytest$.
\end{enumerate}
\noindent
In principle, we could use any clustering and classification methods in
steps~\ref{step:gabriel-cluster} and~\ref{step:gabriel-classify}.  In this
report, we use $k$-means as the clustering algorithm.  For the classification
step, we compute the mean value of $X$ for each class; we assign an
observation to class $g$ if that class has the closest mean (randomly breaking
ties between classes).  The classification step is equivalent to linear
discriminant analysis with equal class priors and identity noise covariance
matrix.


To choose the folds, we randomly partition the rows and columns into $K$ and
$L$ subsets, respectively.  Each fold is indexed by a pair $(r,s)$ of
integers, with $r \in \{1, \dotsc, K\}$ and $s \in \{1, \dotsc, L\}$.  Fold
$(r,s)$ treats the $r$th row subset as ``test'', and the $s$th column subset
as ``response''.  We typically take $K = 5$ and $L = 2$.  For the number of
clusters, we select the value of $k$ that minimizes the average of $\CV(k)$
over all $K \times L$ folds (choosing the smallest value of $k$ in the event
of a tie).



\section{Self-Consistency of Gabriel CV method}


This section gives the self-consistency proof of the proposed Gabriel method.
Specifically, we will show that under appropriate conditions, in the absence
of noise, the Gabriel cross-validation procedure finds the optimal number of
clusters.


Because $k$-means algorithm is essential to the method, we review the
procedure here.  Given a set of observations $\{ x_1, \dotsc ,x_n \}$, and a
specified the number of clusters $k$, the goal of the $k$-means procedure is
to find a set of $k$ or cluster centers $A = \{ a_1, \dotsc, a_k \} $
minimizing the within cluster dispersion
\[
  W(A) = \sum_{i=1}^{n} \min_{a \in A} \|x_i - a\|^2.
\]
This implicitly defines a cluster assignment rule
\[
  g(x) = \argmin_{g \in \{1, \dotsc, k\}} \|x - a_g\|^2,
\]
with ties broken arbitrarily.  We will assume that the $k$-means procedure
finds an optimal solution, $A$, but we will not assume that this solution is
unique.


It will suffice to analyze a single fold of the cross-validation procedure.
As in in section~\ref{sec:gabriel-cv-algorithm} we assume that the $P$
variables of the data set have been partitioned into $p$ predictor variables
represented in vector~$X$ and $q$ response variables represented in
vector~$Y$.  The $N$ observations have been divided into two sets: $n$ train
observations and $m$ test observations.  The following theorem gives
conditions for Gabriel CV to recover the true number of clusters in the
absence of noise.


\begin{theorem}\label{thm:self-consistency}

Let $\{ (X_i, Y_i) \}_{i=1}^{n+m}$ be the data from a single fold of Gabriel
cross-validation.  For any $k$, let $CV(k)$ be the cross-validation error for
this fold, computed as described in Section~\ref{sec:gabriel-cv-algorithm}.
We will assume that there are $K$ true centers $\mu(1), \dotsc,\mu(K)$, with
the $g$th cluster center partitioned as $\mu(g) = \bigl(\muX(g),
\muY(g)\bigr)$ for $g = 1, \dotsc, K$.  Suppose that
\begin{enumerate}[label=(\roman*)]
  \item \label{asn:self-consistency-noiseless}
    Each observation $i$ has a true cluster $G_i \in \{ 1, \dotsc, K \}$.
    There is no noise, so that $X_i = \muX({G_i})$ and $Y_i = \muY(G_i)$ for
    $i = 1, \dotsc, n+m$.

  \item \label{asn:self-consistency-distinct-mux}
    The vectors $\muX(1), \dotsc,\muX(K)$ are all distinct.

  \item \label{asn:self-consistency-distinct-muy}
    The vectors $\muY(1), \dotsc,\muY(K)$ are all distinct.

  \item \label{asn:self-consistency-train}
    The training set contains at least one member of each cluster: for all $g$
    in the range $1, \dotsc, K$, there exists at least one $i$ in the range
    $1, \dotsc, n$ such that $G_i = g$.

  \item \label{asn:self-consistency-test}
    The test set contains at least one member of each cluster: for all $g$ in
    the range $1, \dotsc, K$, there exists at least one $i$ in the range $n+1,
    \dotsc, n+m$ such that $G_i = g$.

\end{enumerate}
Then $CV(k) < CV(K)$ for $k < K$, and $CV(k) = CV(K)$ for $k > K$, so that
Gabriel CV correctly chooses $k = K$.
\end{theorem}

This theorem is implied by the following two lemmas.

\begin{lemma}
Suppose that the assumptions of Theorem~\ref{thm:self-consistency} are in
force.  If $k < K$, then $\CV(k) > 0$.
\end{lemma}
\begin{proof}
By definition,
\[
  \CV(k)
    =
      \sum_{i=n+1}^{n+m}
        \| Y_i - \bmuY (\hGX_i) \|^2,
\]
where $\bmuY(g)$ is the center of cluster $g$ returned from applying $k$-means
to $Y_1, \dotsc, Y_n$.  Assumptions~\ref{asn:self-consistency-noiseless}
and~\ref{asn:self-consistency-test}, imply that as $i$ ranges over the test
set $n+1, \dotsc, n+m$, the response $Y_i$ ranges over all distinct values in
$\{ \muY(1), \dotsc, \muY(K) \}$.
Assumption~\ref{asn:self-consistency-distinct-muy} implies that there are
exactly $K$ such distinct values.  However, there are only $k$ distinct values
of $\bmuY(g)$.  Thus, at least one summand
\(
  \| Y_i - \bmuY(\hGX_i) \|^2
\)
is nonzero.  Therefore,
\(
  \CV(k) > 0.
\)
\end{proof}


\begin{lemma}
Suppose that the assumptions of Theorem~\ref{thm:self-consistency} are in
force.  If $k \geq K$, then $\CV(k) = 0$.
\end{lemma}
\begin{proof}
From assumptions~\ref{asn:self-consistency-noiseless},
\ref{asn:self-consistency-distinct-muy},
and~\ref{asn:self-consistency-train}, we know the cluster centers
gotten from applying $k$-means to $Y_1, \dotsc, Y_n$ must include
$\muY(1), \dotsc, \muY(K)$.  Without loss of generality, suppose that
$\bmuY(g) = \muY(g)$ for $g = 1, \dotsc, K$.  This implies that
$\hGY_i = G_i$ for $i = 1, \dotsc, n$.  Thus, employing
assumption~\ref{asn:self-consistency-noiseless} again, we get that
$\bmuX(g) = \muX(g)$ for $g = 1, \dotsc, K$.


Since assumption~\ref{asn:self-consistency-distinct-mux} ensures that
$\muX(1), \dotsc, \muX(K)$ are all distinct, we must have that $\hGX_i = G_i$
for all $i = 1, \dotsc, m+n$.  In particular, this implies that $\bmuY(\hGX_i)
= Y_i$ for $i = 1, \dotsc, m+n$, so that $\CV(k) = 0$.
\end{proof}



\section{Analysis of Gabriel Cross-Validation with Gaussian Noise}

\subsection{Single cluster in two dimensions}

Now we we analyze the asymptotic performance of Gabriel Cross-Validation, in
the case of Gaussian noise. Our main result is that with single-cluster
Gaussian data, if the predictor and response columns of $\dataX$ are weakly
correlated or independent, then the method will correctly prefer $k = 1$ to
$k = 2$ clusters. We state the result in the case where $\dataX$ has 2 columns,
but it is straightforward to generalize this result to higher dimensions.

\begin{proposition}\label{prop:1clust-2dim}
Suppose that $\{ (X_i, Y_i) \}_{i=1}^{n + m}$ is data from a single fold
of Gabriel cross-validation, where each $(X,Y)$ pair in $\R^2$ is an
independent draw from a mean-zero multivariate normal distribution with unit
marginal variances and correlation $\rho$.  In this case, the data are drawn
from a single cluster; the true number of clusters is~$1$.  If $|\rho| < 0.5$,
then $\CV(1) < \CV(2)$ with probability tending to one as $m$ and $n$ increase.
\end{proposition}

\begin{proof}

Given $(X_1, Y_1), \dotsc, (X_n, Y_n)$, we first apply $k$-means to $\{ Y_i
\}_{i=1}^{n}$. With $k = 1$, the single-cluster centroid will
be equal to $\bar Y_n$, the sample mean of the $Y_1, \dotsc, Y_n$, approximately equal to
$\E(Y) = 0$, with error of size $\OhP(n^{-1/2})$. The cross-validation error
will be
\[
  \CV(1) = \frac{1}{m} \sum_{i=n+1}^{n+m} \| Y_i - \bar Y_n \|^2
         = 1 + \OhP(m^{-1/2}) + \OhP(n^{-1/2}).
\]

Now we will consider the $k = 2$ case.  If $n$ is large enough, then
\citet{pollard1981strong} showed that the centroids $\bmuY_1$ and $\bmuY_2$
will be close to $\E(Y \mid Y > 0) = \sqrt{2/\pi}$ and $\E(Y \mid Y < 0) =
-\sqrt{2/\pi}$.  We have used Lemma~\ref{lem:truncated-normal-moments}
(Appendix~\ref{app:technical-lemmas}) to compute the expectations.  Further,
\citet{pollard1982central} showed that the errors will be of size $\OhP(n^{-1/2})$.


If $\rho > 0$ and $n$ is large enough, then classification rule learned from
$\{(X_i, \hGY_i)\}_{i=1}^{n}$ variables will be determined according to
whether $X > 0$; if $\rho < 0$ then the decision is according to whether $X <
0$.  More specifically, the decision boundary will be at $0 + \OhP(n^{-1/2})$.

In the $\rho > 0$ case, the cross-validation error will be
\begin{align*}
  \CV(2)
  &=
    \frac{1}{m}
    \sum_{i=n+1}^{n+m}
      \| (Y_i - \bmuY_1) 1\{\hGX_i = 1\} \|^2
      +
      \| (Y_i - \bmuY_2) 1\{\hGX_i = 2\} \|^2
\\
  &=
    \E[(Y - a)^2 1\{ X > 0\}] + \E[(Y + a)^2 1\{X < 0\}]
      + \OhP(m^{-1/2}) + \OhP(n^{-1/2}),
\end{align*}
where $a = \sqrt{2/\pi}$.  From the joint normality of $X$ and $Y$, it follows
that $Y \mid X$ is normal with mean $\rho X$ and variance $(1 - \rho^2)$, so
that $\E[(Y - a)^2 \mid X] = (\rho X - a)^2 + (1 - \rho^2)$.  Applying
Lemma~\ref{lem:truncated-normal-moments}, we get that for large $m$ and $n$,
the Gabriel cross-validation error is close to
$1 + a^2 (1 - 2 \rho)$.  

In the $\rho < 0$ case, a similar calculation shows that $\CV(2)$ is close to
$1 + a^2 (1 + 2 \rho)$.  In particular, if $|\rho| < 0.5$, then with
probability tending to $1$ and $m$ and $n$ increase, the asymptotic
cross-validation error for $k = 1$ will be smaller than for $k = 2$.
\end{proof}


We confirm this result with a simulation.  We perform $10$ replicates.  In each
replicate, we generate $20000$ observations from a mean-zero bivariate normal
distribution with unit marginal variances and correlation $\rho$.  We perform
a single $2 \times 2$ fold of Gabriel cross-validation and report the
cross-validation mean squared error for the number of clusters $k$ ranging
from $1$ to $5$.  Figure~\ref{fig:nullcorr-equal} shows the cross-validation
errors for all $10$ replicates.  The simulation demonstrates that in the
Gabriel cross-validation criterion chooses the correct answer $k = 1$ whenever
$\rho < 0.5$; the criterion chooses $k = 2$ clusters whenever $|\rho| > 0.5$.

\begin{figure}[H]
\centering
\includegraphics[width=25pc]{demo/nullcorr/equal.pdf}
\caption{Cross-validation error on $10$ replicates, with the number of
clusters $k$ ranging from $1$ to $5$.  Data is generated from two-dimensional
multivariate normal distribution with correlation $\rho$.  The Gabriel
cross-validation criterion chooses the correct answer $k = 1$ whenever
$|\rho| < 0.5$; the criterion chooses $k = 2$ clusters whenever $|\rho| > 0.5$.}
\label{fig:nullcorr-equal}
\end{figure}


\subsection{Single cluster in higher dimensions}

To generalize the result of the previous section to higher dimensions, 
suppose that $\{ (X_i, Y_i) \}_{i=1}^{n+m}$ is data from a single fold of
Gabriel Cross-Validation, with each $(X, Y)$ pair in $\R^p \times \R^q$ a
draw a mean-zero multivariate normal distribution. To analyze this setting, it
suffices to consider the mean-zero case with $\cov(X) = I_p$.

Suppose that $\cov(Y) = U \Lambda U^\T$ is the eigendecomposition of
$\cov(Y)$. For simplicity, suppose that the leading eigenvalue $\lambda_1$
has unit multiplicity, so that the first principal subspace of $Y$ is a line,
given by the span of the vector $u_1 \in \R^q$. When we apply $k$-means to
$Y_1, \dotsc, Y_n$ with $k = 2$, the centroids will be close to
$\bmuY_1 =  \sqrt{2\lambda_1 /\pi} u_1$ and
$\bmuY_2 = -\sqrt{2\lambda_1 /\pi} u_1$, with errors of size $\OhP(n^{-1/2})$.

The means for the $X$ variables will be close to
$\bmuX_1 = \E(X \mid u_1^\T Y > 0)$ and
$\bmuX_2 = \E(X \mid u_1^\T Y < 0)$,
with errors of size $\OhP(n^{-1/2})$.  Let
\[
  \begin{bmatrix}
    I_p & \lambda_1^{1/2} \rho \\
    \lambda_1^{1/2} \rho^T & \lambda_1
  \end{bmatrix}
\]
be the covariance matrix of $(X, u_1^\T Y)$,
where $\rho = (\rho_1, \dotsc, \rho_p)$ is such that $\rho_i = \corr(X_i,
u_1^\T Y)$ for $i = 1, \dotsc, p$.
. Then,
\[
  \bmuX_1 = \sqrt{2 \lambda_1 / \pi} \rho,
  \qquad
  \bmuX_2 = -\sqrt{2 \lambda_1 / \pi} \rho;
\]
thus, the classification rule learned from $\{ (X_i, \hGY_i) \}_{i=1}^{n}$
will send observations to class $1$ or $2$ according to whether
$\rho^\T X + \OhP(n^{-1/2})$ is positive or negative.


We have reduced the analysis of cross-validation error to the two-dimensional
case. In particular, following the calculations from the proof of
Prop.~\ref{prop:1clust-2dim}, if $|\corr(\rho^\T X, u_1^\T Y)| < 0.5$, then with
probability tending to one as $m$ and $n$ increase, $\CV(1) < \CV(2)$.
The condition $|\corr(\rho^\T X, u_1^\T)| < 0.5$ reduces to
\[
  \sum_{i=1}^p \rho_i^2 < 0.25.
\]
A sufficient (but not necessary) condition to ensure that this holds is that
the magnitude of the first canonical correlation between the $X$ variables and
the $Y$ variables is below $0.5 / \sqrt{p}$.


\subsection{Two clusters}

We will now analyze a simple two-cluster setting, and derive conditions for
Gabriel cross-validation to correctly prefer $k=2$ clusters to $k=1$.

\begin{proposition}
Suppose that $\{(X_i,Y_i)\}_{i=1}^{n+m}$ is data from a single fold of Gabriel
cross-validation, where each $(X,Y)$ pair in $\R^2$ is an independent draw
from an equiprobable mixture of two multivariate normal distributions with
identity covariance. Suppose that the first mixture component has mean $(\muX,
\muY)$, and the second has mean $(-\muX, -\muY)$, where $\muX > 0$ and
$\muY > 0$.  If $1 + 2\Phi(\mu^Y)+ \frac{2\varphi(\mu^Y)}{\mu^Y} < 4\Phi(\mu^X)$, then $\CV(2) < \CV(1)$ with
probability tending to one as $m$ and $n$ increase.
\end{proposition}

\begin{proof}
There are two clusters $G_1$ and $G_2$, where observations from $G_1$ are distributed as
\[	\mathcal{N}\left( \begin{pmatrix} 
    \mu^X \\
    \mu^Y \\
  \end{pmatrix}, \mathbf{I} \right)	\]
and observations from $G_2$ are distributed as
\[	\mathcal{N}\left( \begin{pmatrix} 
    -\mu^X \\
    -\mu^Y \\
  \end{pmatrix}, \mathbf{I} \right)	\]
where $\mu^X_1 > 0$ and $\mu^Y_1 > 0$. Let $G_i$ be the true cluster where observation $i$ is generated from, by assumption
\[	P(G_i=G_1) = P(G_i=G_2) = 1/2	\]
After applying $k$-means on $\{ Y_i\}_{i=1}^{n}$ with $k=2$, if $n$ is large enough, we have the estimated centroids $\bar{\mu}^Y_1$ and $\bar{\mu}^Y_2$ be close to $E(Y \mid Y>0)$ and $\E(Y \mid Y < 0)$,
with errors will be of size $\OhP(n^{-1/2})$. Here
\begin{align}
E(Y \mid Y>0) &= E(Y_1 \mid Y_1 > 0) \cdot P(Y_1>0) + E(Y_2 \mid Y_2 > 0) \cdot P(Y_2>0) \nonumber \\ 
   &= 2\varphi(\mu^Y)+2\mu^Y\Phi(\mu^Y)-\mu^Y  \\  \nonumber 
\end{align}
where $Y_1 \sim N(\mu^Y, 1)$ and $Y_2 \sim N(-\mu^Y, 1)$, and we used Lemma~\ref{lem:truncated-normal-moments}
(Appendix~\ref{app:technical-lemmas}). Similarly, we have
\begin{align}
E(Y \mid Y<0) &= E(Y_1 \mid Y_1 < 0) \cdot P(Y_1<0) + E(Y_2 \mid Y_2 < 0) \cdot P(Y_2<0) \nonumber \\ 
   &= -2\varphi(\mu^Y)-2\mu^Y\Phi(\mu^Y)+\mu^Y  \\  \nonumber 
\end{align}
 where $\varphi()$ and $\Phi()$ are the standard normal probability and cumulative distribution function respectively.  


Same as in single cluster case, the classification rule learned from
$\{(X_i, \hGY_i)\}_{i=1}^{n}$ variables will be determined according to
whether $X > 0$, with the decision boundary be at $0 + \OhP(n^{-1/2})$.
By symmetry, the CV error for points from $G_1$ is same as the points from $G_2$. Because $P(G=G_1) = P(G=G_2) =1/2$, the CV error can be calculated solely from $G_2$, that is

\begin{align*}
  \CV(2)
  &=
    \frac{1}{m}
    \sum_{i=n+1}^{n+m}
      \| (Y_i - \bmuY_1) 1\{\hGX_i = 1\} \|^2
      +
      \| (Y_i - \bmuY_2) 1\{\hGX_i = 2\} \|^2, \hspace{0.2in}Y\sim N(-\mu^Y,1)
\\
  &=
    \E[(Y - a)^2 1\{ X > 0\}] + \E[(Y + a)^2 1\{X < 0\}]
      + \OhP(m^{-1/2}) + \OhP(n^{-1/2})\\
   \end{align*}
   With 
   \begin{align}
 \E[(Y - a)^2 1\{ X > 0\}] + \E[(Y + a)^2 1\{X < 0\}]  =& P(\hGX_i = 1)\cdot E[(Y - a)^2] + P(\hGX_i = 2)\cdot E[(Y + a)^2] \nonumber \\  \nonumber
 =& [1-\Phi(\mu^X)][var(Y)+(-\mu^Y - a)^2]+\\ \nonumber
  & \Phi(\mu^X)[var(Y)+(-\mu^Y + a)^2]\\  \nonumber
  =& [1-\Phi(\mu^X)][1+(\mu^Y + a)^2]+\Phi(\mu^X)[1+(\mu^Y -a )^2]   \\  \nonumber
  =& 1+(\mu^Y + a)^2 -4a\Phi(\mu^X)\mu^Y  \nonumber
\end{align}
where $a$ is given by $(1)$.


When $k=1$, the result is straight forward since the estimated centroid will approximately equal to
$0$, with error of size $\OhP(n^{-1/2})$. The cross-validation error
will be
\[
  \CV(1) = \frac{1}{m} \sum_{i=n+1}^{n+m} \| Y_i - \bar Y_n \|^2
         = 1 + (\mu^Y)^2 + \OhP(m^{-1/2}) + \OhP(n^{-1/2}).
\]
So if we have $1 + 2\Phi(\mu^Y)+ \frac{2\varphi(\mu^Y)}{\mu^Y} < 4\Phi(\mu^X)$, we have $\CV(2) < \CV(1)$
\end{proof}

We confirm this result with a simulation.  We perform $10$ replicates for each pair of $(\mu^X, \mu^Y)$, where both $\mu^X$ and $\mu^Y$ take value on grid of $[0,3]$ with step $0.1$.  In each
replicate, we generate $20000$ observations from two multivariate normal distributions with identity covariance, where one has mean $(\muX, \muY)$ and the other one has mean $(-\muX, -\muY)$.
We perform a single $2 \times 2$ fold of Gabriel cross-validation and report the
times (out of $10$ replicates) when $k=2$ is selected by the algorithm in stead of $k=1$. Figure~\ref{fig:overlap-color_plot} shows the frequency $k=2$ is selected by the algorithm for each pair of $(\mu^X, \mu^Y)$. The red spot means high number (close to $10$) is selected by the algorithm, which means algorithm very likely will pick $k=2$ over $k=1$ for the corresponding $(\mu^X, \mu^Y)$. While blue spot means algorithm prefer $k=1$ for the corresponding value of $(\mu^X, \mu^Y)$. We can see the simulation result perfectly align with the theoretical curve (the black line), which separates the $k=2$ zone from the $k=1$ zone. It demonstrates that the Gabriel cross-validation works exactly as it suppose to under such setting. The position of red spots shows that when the two clusters are reasonably apart (not overlapping too heavily) in both dimensions, the Gabriel cross-validation is asymptotically consistent.

\begin{figure}[H]
\centering
\includegraphics[width=25pc]{demo/overlap/color_plot.pdf}
\caption{Number of times $k=2$ is selected out of $10$ replicates for each pair of $(\muX, \muY)$. The heat map shows the frequency $k=2$ is selected by the algorithm, with blue means low number (of $k=2$) is selected and red color means high number is selected. The black line is the theoretical curve, above which the algorithm suppose to pick $k=2$ and below which algorithm select $k=1$.  }
\label{fig:overlap-color_plot}
\end{figure}




\section{Simulation}

In this section, simulation is performed to evaluate the performance of our
proposed methods in locating the ``correct" number of clusters. We compare
with a basket of existing methods including Gap statistics
\citep{tibshirani2001estimating}, Gaussian mixture model-based clustering
\citep{fraley2002model}, CH-index \citep{calinski1974dendrite}, Hartigan
statistics \citep{hartigan1975clustering}, Jump method
\citep{sugar2003finding}, Prediction strength \citep{tibshirani2005cluster},
Bootstrap stability \citep{fang2012selection} in following simulation
settings. We select $p=q$ and $5-$fold cross-validation in row
($m=\frac{1}{4}n$) as default parameter setting for our proposed Gabriel
method. Note that set $p=q$ corresponding to $2-$fold cross-validation in
column.
\begin{enumerate}

  \item Single cluster in $10$ dimensions --- $200$ observations, each
    observation uniformly distributed over $[0,1]$ in each dimension. 

  \item Two clusters in $4$ dimensions --- $50$ i.i.d
    observations are generated from both multivariate normal
    $\mathcal{N}\left(\mu_1,0.5\Sigma\right)$ and multivariate normal
    $\mathcal{N}\left(\mu_2,1.5\Sigma\right)$, where 
    cluster center $\mu_1 = (1,0,0,1)$ and $\mu_2 =(1,3.5,3.5,1)$
    respectively. $\Sigma$ has $AR(1)$ structure with $\rho=-0.2$ and $\sigma=1$.

  \item Four clusters in $100$ dimensions ---  Each cluster has $100$ or $150$
    i.i.d standard normal observations, with cluster centers randomly
    generated from multivariate normal distribution
    $\mathcal{N}\left(\mathbf{0},0.65^2\mathbf{I}\right)$

  \item Ten clusters in $100$ dimensions ---  Each cluster has $50$ or $100$
    i.i.d standard normal observations, with cluster centers randomly
    generated from multivariate normal distribution
    $\mathcal{N}\left(\mathbf{0},0.72^2\mathbf{I}\right)$

  \item Four log-normal clusters in $16$ dimensions --- For each cluster, $30$
    or $60$ i.i.d centered log-normal observations are generated from
    $ln\mathcal{N}\left(0,0.5^2 \right)$. The cluster centers are randomly
    generated from multivariate normal distribution
    $\mathcal{N}\left(\mathbf{0},1.2^2\mathbf{I}\right)$

  \item[6.] Three exponential clusters in $20$ dimensions with different
    variance --- $40$ observations in each cluster are generated from centered
    exponential distribution ($exp(\lambda)-1/\lambda$ in each dimension),
    with $\boldsymbol\mu$ randomly generated from
    $\mathcal{N}\left(\mathbf{0},19\mathbf{I}\right)$ and $\lambda = 1, 1/2,
    1/5$ respectively.

  \item[7.] One cluster in $10$ dimensions with correlated noise -- $200$
    mean-zero multivariate normal observations; the covariance matrix $\Sigma$
    has $\Sigma_{ij} = 0.9$ for $i \neq j$ and $\Sigma_{ii} = 1$ for $i = 1,
    \dotsc, 10$.
\end{enumerate}
Note that in setting 2--6, all clusters are well-separated, i.e. no
overlapping. In fact, any simulated clusters with minimum distance less than
1 unit was discarded, so there is clear definition of true number of
clusters. The parameters in setting 2--6 are chosen such that about half of
the random realization were discarded. The idea is borrowed from
\cite{tibshirani2001estimating}.

Table \ref{table1} shows the distribution of $k$ selected by each algorithm in 
each simulation setting. We run $k$ from $1$ to $15$, and the best $k$ within this range
is selected by the algorithms. [ NA in the table means...]
\clearpage

\begin{center}
\footnotesize
\begin{longtable}{lcccccccccccccccc}
\captionsetup{justification=centering}
\caption{\label{table1} Simulation Results}\\
\toprule
\phantom{Pred.~Strength}
  & \multicolumn{16}{c}{Estimated Number of Clusters} \\
\cmidrule(lr){2-17}
Method
  & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & NA \\
\midrule
\endfirsthead
\multicolumn{17}{c}%
  {\tablename\ \thetable\ -- \textit{Continued from previous page}} \\
\midrule
\phantom{Pred.~Strength}
  & \multicolumn{16}{c}{Estimated Number of Clusters} \\
\cmidrule(lr){2-17}
Method
  & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & NA \\
\midrule
\endhead
\multicolumn{17}{r}{\textit{Continued on next page}} \\
\endfoot
\bottomrule
\multicolumn{17}{l}{\footnotesize
  \textsuperscript{$\ast$} CH, Hartigan, and Stability are excluded from
  Setting~1 because they cannot select $k=1$.
}
\endlastfoot
\input{demo/bench/setting1/results.tex}
\midrule
\input{demo/bench/setting2/results.tex}
\midrule
\input{demo/bench/setting3/results.tex}
\midrule
\input{demo/bench/setting4/results.tex}
\midrule
\input{demo/bench/setting5/results.tex}
\midrule
\input{demo/bench/setting6/results.tex}
\midrule
\input{demo/bench/setting7/results.tex}
\end{longtable}
\end{center}



\noindent
All methods are used with their default parameter setting except for Gap
statistics. We selected the $k$  corresponds to the global maximum of gap
statistics, rather than the default method which selects the smallest $k$ such
that the gap statistics is not more than 1 standard error away from the first
local maximum. This is because among all the possible options, the global
maximum criteria gives the best result in our simulation settings.


For CH-index, Hartigen and Bootstrap stability, the minimum $k$ they can pick is $2$. So they are not
included for comparison since the true $k=1$. It also demonstrates the difficulty 
that null scenario poses for some existing algorithms. For those could select $k=1$,
most of them work well under such setting except for Jump method. Setting $2-4$ have Gaussian
distribution, where the Gaussian mixture model-based method suppose to
perform the best. However, it only performs reasonably well in setting $2$. In setting $3$ and $4$,
it has a very poor performance even if the underlying distribution is indeed
Gaussian mixture model. It highlights the difficulty that high dimension
data could cause. Other methods (CH-index, Hartigan, Jump
method, Prediction strength and Boostrap stability) also have problems in
these two high-dimension settings as we could not find any one with reasonable
performance in terms of finding the correct $k$ in both cases. The only exception is
Gap statistics, which works perfectly in both settings. Our proposed Gabriel method (also Wold
CV method) clearly stands out from the basket of existing methods under the 
high-dimension setting $3$ and $4$, as we can see they both perform perfectly under
such settings. 


Setting $5$ and $6$ corresponding to the situation where data are heavy-tailed
instead of normal distributed, coupled with unequal number of observations or
unequal variance for different clusters. From Table \ref{table1}, we can see none of 
the existing methods has good performance in both settings. In contrast,
our proposed Gabriel method is robust and has superior performance
compare with other methods. Because one can hardly tell the underlying
distribution of data in practice, the resilience to non-Gaussian data gives
the Gabriel method a clear edge. 



\section{Real data application}

We also applied our proposed method to three real world data sets obtained
from the University of California Irvine machine learning repository. The
first and third data sets are selected because there are clear number of
clusters in those two data sets. The second data set is used as a benchmark
data set since it was widely used in literature.


The first one is congress voting data which consists of voting records of
$98$th United States Congress, $2$nd session \citep{schlimmer1987concept}. 
This data set includes votes for each of the U.S. House of Representatives Congressmen on the $16$ key votes
identified by the \textit{CQA} (Congressional Quarterly Almanac). For each
vote, each Congressman either vote positively ``yea" (voted for/paired
for/announced for),  negatively ``nay'' (voted against/paired
against/announced against) or position unknown ``?". We took out those records
contain ``?" before comparing each algorithm. It results in $232$ remaining
records, with $124$ democrat and $108$ republican. 


The second data set is the well-known Wisconsin breast cancer data set \citep{mangasarian1990pattern}. After excluding the records with missing data, this data set consists records of $683$ patients, each with measurements of nine attributes of their biopsy specimens. It is known that there exist at least two groups of patients: $444$ patients with benign specimens and $239$ patients with malignant specimens.

The third data set is the Sonar data returned from two targets -- a metal
cylinder and a rock with similar shape, which is first studied by
\cite{gorman1988analysis} using a neural network. Both targets were impinged
by pulse which was a wide-band linear FM chirp $(ka = 55.6)$. Returns were
collected at a range of $10$ meters and obtained from the cylinder at aspect
angles spanning $90^{\circ}$ and from the rock at aspect angles spanning
$180^{\circ}$. The data set contains $208$ returns ($111$ cylinder returns and
$97$ rock returns), with each composed of $60$ spectral samples, normalized to
take on values between $0$ and $1$. So the data has $60$ features with clearly
$2$ clusters. 


\begin{table}[H]
\begin{center}
\captionsetup{justification=centering}
\caption{\label{table2} Number of clusters selected by each algorithm}
\begin{tabular}{lccccc}
    \hline                  
 & Congress Voting && Breast Cancer && Sonar \\ \hline                    
CH-index & $2$ && $2$  && $2$    \\
Hartigan & $3$ && $3$  &&  $3$   \\
Jump & $9$ && $9$ && $10$  \\   
Prediction strength & $2$ &&  $2$ && $1$    \\
Bootstrap stability & $2$ &&  $2$ &&  $10$   \\ 
Gap & $10$ && $9$ && $10$  \\   
Gaussian-Mix & $7$ && $5$ && $1$  \\   
Gabriel & $2$  && $3$  && $2$  \\    
Wold & $2$ && $3$ && $10$ \\  \hline 
\end{tabular}
\end{center}
\hspace{0.5in} \footnotesize {All the algorithms executed with their default parameter settings with $k$ ranges from $1$ to $10$}
\end{table} 

Since most congressmen vote base on their parties' interest, $2$ parties
(Democratic and Republican) represent two clusters in this data set. So the
optimal number should be two. Close inspection shows $k$-means with
$k=2$ separates the two parties very well with the lowest miss-classification
error $10.43\%$ ($k=3$ has $14.78\%$). Note that CH-index and Bootstrap
stability also return $k=2$, but $2$ is the lower bound those methods can
select for $k$. So it's not clear they actually choose $k=2$ or they hit the
lower bound (they would pick $k=1$ if allowed). For the breast cancer data,
it's known to have at least $2$ cluster based on whether it's benign specimens
or not. But it doesn't mean the optimal $k$ should be $2$.
\cite{fujita2014non} noticed that the malign group is quite heterogeneous and
can be further clustered into at least two subgroups. Hence, the result $k=3$
given by our proposed Gabriel method (as well as Wold method) make sense. For
the Sonar data set which is relatively high-dimension, only our proposed
Gabriel method and CH-index selects $k=2$. Majority of the methods collapse to
either select the maximum or select the minimum number, underline how
difficult it was to pick the right $k$ when dimension increases. 


\section{Conclusion}

In this paper, we proposed a novel approach to estimate the number of
clusters. The intuition behind our proposed methods is to transfer the
unsupervised learning problem into supervised learning problem via novel form
of cross validation. We proved the self-consistency for our proposed Gabriel
CV method as well as its asymptotic property with noise, and showed the
robustness of our method by simulation. Our method has very good performance
in our limited simulation settings, and clearly the superior one when data is
high dimension or is heavy-tailed.


\section*{\textbf{APPENDIX}}
\appendix
\section{Wold CV estimation} \label{app:foobar}
\begin{itemize}
	\item For each $k = 1,2,...,k_{max}$
	\begin{enumerate}
		\item Randomly draw some entries in $\mathbf{X}$ missing, keep those hold-out values in vector $V_{true}$
		\item Impute the missing values with column mean or $0$, denote the imputed data as $\mathbf{X}_{new}$
		\item Apply the iterative procedure below until converge or stopping criteria reached
		\begin{itemize}
			\item Apply $K$-mean on data set $\mathbf{X}_{new}$ with parameter $k$
			\item Substitute each observation in $\mathbf{X}_{new}$ by its nearest center, get new data $\mathbf{X}^c_{new}$ ($\mathbf{X}_{new}$ keep the same)
			\item Replace (impute) those imputed values in $\mathbf{X}_{new}$ with the corresponding entries in $\mathbf{X}^c_{new}$
			\item Calculate the difference between the old and newly imputed values, check whether or not they coincide (converge) 
		\end{itemize}
		\item Obtain the last imputed entry values of converged $\mathbf{X}_{new}$, denote it by $V_{converge}$
		\item Calculate the prediction error  $Error_k = ||V_{true} - V_{converge}||^2$ 
	\end{enumerate}
	\item For each CV folder, repeat above procedure and obtain the $Error_k$ for each $k$
	\item Average $Error_k$ across all folders for each $k$, and then select the $k$ corresponding to the minimum average $Error_k$ 
\end{itemize}


\section{Technical Lemmas}
\label{app:technical-lemmas}

\begin{lemma}\label{lem:truncated-normal-moments}

If $Z$ is a standard normal random variable, then
\[
  \E(Z \mid a < Z < b)
    = - \frac{\varphi(b) - \varphi(a)}
             {\Phi(b) - \Phi(a)}
\]
and
\[
  \E\{(Z - \delta)^2 \mid a < Z < b\}
    = \delta^2 + 1
    - \frac{  (b - 2 \delta) \varphi(b)
            - (a - 2 \delta) \varphi(a)}
           {\Phi(b) - \Phi(a)}
\]
for all constants $a$, $b$, and $\delta$, where $\varphi(z)$ and $\Phi(z)$ are
the standard normal probability density and cumulative distribution functions.
These expressions are valid for $a = -\infty$ or $b = \infty$ by taking
limits.

\end{lemma}
\begin{proof}
We will derive the expression for the second moment.  Integrate to get
\begin{align*}
  \E[ (Z - \delta)^2 1\{Z < b\}]
    &= \int_{-\infty}^b (z - \delta)^2 \varphi(z) \, dz \\
    &= (\delta^2 + 1) \Phi(b) - (b - 2 \delta) \varphi(b).
\end{align*}
Now,
\[
  \E\{(Z - \delta)^2 \mid a < Z < b\}
    =
    \frac{  \E[ (Z - \delta)^2 1\{Z < b\}]
          - \E[ (Z - \delta)^2 1\{Z < a\}]}
         { \Phi(b) - \Phi(a) }.
\]
\end{proof}

Lemma~\ref{lem:truncated-normal-moments} has some important special cases:
\begin{align*}
  \E\{Z \mid Z > 0\} &= 2 \varphi(0) = \sqrt{2 / \pi}, \\
  \E\{(Z - \delta)^2 \mid Z > 0 \}
    &= \delta^2 + 1 - 4 \delta \varphi(0), \\
  \E\{(Z - \delta)^2 \mid Z < 0 \}
    &= \delta^2 + 1 + 4 \delta \varphi(0).
\end{align*}



\bibliography{references}
\bibliographystyle{apalike}
\end{document}
