\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 
%---------We added packages---------------------
%\usepackage{epsf}
\usepackage{hyperref}
%\usepackage{graphicx}
\usepackage{subfigure}
%\usepackage[margin=2.5cm]{geometry}
%\usepackage{amsmath}
\usepackage{footnote}
\usepackage[bottom]{footmisc}
\usepackage[singlelinecheck=false]{caption}
%\usepackage{caption}
\usepackage{amsthm}
\usepackage{enumitem}
%\usepackage{graphics}
\usepackage{float}
\usepackage{amssymb}
%\usepackage{natbib}
\usepackage{lscape}
\usepackage{array}
%\textheight=8.7in
\usepackage{setspace}
\usepackage{verbatim}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{longtable}

%\def\threedigits#1{%
%  \ifnum#1<100 0\fi
%  \ifnum#1<10 0\fi
%  \number#1}

%\topmargin=0.1in \oddsidemargin=-0.1cm \evensidemargin=-0.1cm

%\paperheight=11in \paperwidth=8.5in \marginparwidth=0in

%\marginparsep=0in \textwidth=6.5in \headheight=0in \headsep=0in

\onehalfspacing
\def\argmax{\mathop{\rm arg\,max}}
%\usepackage{xspace,epsfig,subfig}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newenvironment{sketch}{\noindent\emph{Proof Sketch:}}{$\quad \Box$}
%\newenvironment{proof}{\noindent\emph{Proof:}}{$\quad \Box$}

\bibpunct{(}{)}{;}{a}{,}{,}

%----------------------We added packages----------------------------
\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%


\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf Estimating the number of clusters using Cross Validation}
  \author{Wei Fu \thanks{
    The authors gratefully acknowledge}\hspace{.2cm}\\
    Department of IOMS, New York University\\
    and \\
    Patrick O. Perry \\
    Department of IOMS, New York University}
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Estimating the number of clusters using Cross Validation}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
Many clustering methods, including $k$-means, require the user to specify the
number of clusters as an input parameter. A variety of methods have been
devised to choose the number of clusters automatically, but they often rely on
strong modeling assumptions. We propose a data-driven approach to estimate the
number of clusters based on a novel form of cross-validation. This differs
from ordinary cross-validation, because clustering is fundamentally an
unsupervised learning problem. Simulation and real data analysis results show
that our proposed method outperforms existing methods, especially in
high-dimensional settings with heavy-tailed data.
\end{abstract}

\noindent%
{\it Keywords:}  clustering, unsupervised learning, data-driven method 
\vfill
\hfill {\tiny technometrics tex template (do not remove)}

\newpage
\spacingset{1.45} % DON'T change the spacing!


%------------------------------------------------------------------
%-------------------------------------------------------------
\section{Analysis of Gabriel Cross-Validation with Gaussian Noise}


\begin{proposition}
Suppose that $\{ (X_i, Y_i) \}_{i=1}^{n + m}$ is data from a single fold
of Gabriel cross-validation, where each $(X,Y)$ pair in $\R^2$ is an
independent draw from a mean-zero multivariate normal distribution with unit
marginal variances and correlation $\rho$.  In this case, the data are drawn
from a single cluster; the true number of clusters is~$1$.  If $|\rho| < 0.5$,
then $\CV(1) < \CV(2)$ with probability tending to one as $m$ and $n$ increase.
\end{proposition}

\begin{proof}
Given $(X_1, Y_1), \dotsc, (X_n, Y_n)$, we first apply $k$-means to $\{ Y_i
\}_{i=1}^{n}$. With $k = 1$, the single-cluster centroid will
be equal to $\bar Y_n$, the sample mean of the $Y_1, \dotsc, Y_n$, approximately equal to
$\E(Y) = 0$, with error of size $\OhP(n^{-1/2})$. The cross-validation error
will be
\[
  \CV(1) = \frac{1}{m} \sum_{i=n+1}^{n+m} \| Y_i - \bar Y_n \|^2
         = 1 + \OhP(m^{-1/2}) + \OhP(n^{-1/2}).
\]

Now we will consider the $k = 2$ case.  If $n$ is large enough, then
\citet{pollard1981strong} showed that the centroids $\bmuY_1$ and $\bmuY_2$
will be close to $\E(Y \mid Y > 0) = \sqrt{2/\pi}$ and $\E(Y \mid Y < 0) =
-\sqrt{2/\pi}$.  We have used Lemma~\ref{lem:truncated-normal-moments}
(Appendix~\ref{app:technical-lemmas}) to compute the expectations.  Further,
\citet{pollard1982central} showed that the errors will be of size $\OhP(n^{-1/2})$.

If $\rho > 0$ and $n$ is large enough, then classification rule learned from
$\{(X_i, \hGY_i)\}_{i=1}^{n}$ variables will be determined according to
whether $X > 0$; if $\rho < 0$ then the decision is according to whether $X <
0$.  More specifically, the decision boundary will be at $0 + \OhP(n^{-1/2})$.

In the $\rho > 0$ case, the cross-validation error will be
\begin{align*}
  \CV(2)
  &=
    \frac{1}{m}
    \sum_{i=n+1}^{n+m}
      \| (Y_i - \bmuY_1) 1\{\hGX_i = 1\} \|^2
      +
      \| (Y_i - \bmuY_2) 1\{\hGX_i = 2\} \|^2
\\
  &=
    \E[(Y - a)^2 1\{ X > 0\}] + \E[(Y + a)^2 1\{X < 0\}]
      + \OhP(m^{-1/2}) + \OhP(n^{-1/2}),
\end{align*}
where $a = \sqrt{2/\pi}$.  From the joint normality of $X$ and $Y$, it follows
that $Y \mid X$ is normal with mean $\rho X$ and variance $(1 - \rho^2)$, so
that $\E[(Y - a)^2 \mid X] = (\rho X - a)^2 + (1 - \rho^2)$.  Applying
Lemma~\ref{lem:truncated-normal-moments}, we get that for large $m$ and $n$,
the Gabriel cross-validation error is close to
$1 + a^2 (1 - 2 \rho)$.  

In the $\rho < 0$ case, a similar calculation shows that $\CV(2)$ is close to
$1 + a^2 (1 + 2 \rho)$.  In particular, if $|\rho| < 0.5$, then with
probability tending to $1$ and $m$ and $n$ increase, the asymptotic
cross-validation error for $k = 1$ will be smaller than for $k = 2$.
\end{proof}



Proposition $1$ gives a very simple condition for the Gabriel method to correctly 
pick $k=1$ with single cluster in $2$ dimensions. The following proposition generalizes
such condition for data in arbitrary dimension. 


\begin{proposition}
Suppose that $\{ (X_i, Y_i) \}_{i=1}^{n + m}$ is data from a single fold
of Gabriel cross-validation, where each $(X,Y)$ pair in $\R^{p+q}$ is an
independent draw from a mean-zero multivariate normal distribution with 
covariance matrix $\Sigma_{XY}= \left( \begin{smallmatrix} \Sigma_{XX} & \Sigma_{XY} \\ 
 \Sigma_{YX} & \Sigma_{YY} \end{smallmatrix}\right)$, with $\Sigma_{YY}$ has leading 
eigenvalue $\lambda_1$ and corresponding eigenvector $u_1$. In this case, the data are drawn
from a single cluster; the true number of clusters is~$1$.  If  $\frac{\sqrt{\lambda_1}}{2}
 > \frac{u^T_1\Sigma_{YX}\Sigma_{XY}u_1}{\sqrt{u^T_1\Sigma_{YX} \Sigma_{XX} \Sigma_{XY} u_1}}$,
then $\CV(1) < \CV(2)$ with probability tending to one as $m$ and $n$ increase.
\end{proposition}

\begin{proof}
Let $X$ and $Y$ be jointly multivariate normal distributed with mean $\mathbf{0}$ and covariance matrix $\Sigma_{XY}$, i.e.
\[	(X,Y) \sim \mathcal{N} \left( \mathbf{0}, \Sigma_{XY}\right)	\]
where $\Sigma_{XY}=\begin{bmatrix} \Sigma_{XX} & \Sigma_{XY} \\  \Sigma_{YX} & \Sigma_{YY} \end{bmatrix}$.

Let $\Sigma_{YY} = U \Lambda U^T$  be the eigendecomposition of $\Sigma_{YY}$, with leading eigenvalue $\lambda_1$ and corresponding eigenvector $u_1$. Then the centroid of $k$-means applying on $(y_1,..,y_n)$ is on the first PC 
of $Y$,\[	E(u^T_1 Y|u^T_1 Y>0) = \bar{\mu}^Y_1 =\sqrt{2 \lambda_1/\pi}u_1\] and 
\[	E(u^T_1 Y|u^T_1 Y<0) = \bar{\mu}^Y_2 =-\sqrt{2 \lambda_1/\pi}u_1\]
 where $u^T_1 Y \sim \mathcal{N}(0,\lambda_1)$.

To compute $\bar{\mu}^X_1 = E(X|u^T_1 Y>0)$, we need to know the conditional distribution $X|u^T_1 Y$. Since $(X,Y)$ has multivariate normal distribution, $(X,u^T_1 Y)$ also has a multivariate normal distribution with mean $\mathbf{0}$ and covariance matrix
$$\Sigma_{X,u^T_1 Y}=\begin{bmatrix} \Sigma_{XX} & \Sigma_{XY} u_1 \\  u^T_1 \Sigma_{YX} & \lambda_1 \end{bmatrix}$$
The conditional distribution $X|u^T_1 Y$ is hence normal with mean
 $$\mu_{X|u^T_1 Y} = \Sigma_{XY} u_1 \lambda^{-1}_1 u^T_1 Y $$
Therefore, 
\begin{align}
\bar{\mu}^X_1 &= E(X \mid u^T_1 Y>0) \nonumber \\ \nonumber
 			  &= E\left(E[X \mid u^T_1 Y] \mid u^T_1 Y>0\right) \\ \nonumber
 			  &=  E\left(\Sigma_{XY} u_1 \lambda^{-1}_1 u^T_1 Y \mid u^T_1 Y>0\right)\\ \nonumber
 			  &= \lambda^{-1}_1 \Sigma_{XY}u_1 E(u^T_1 Y \mid u^T_1 Y>0) \\ \nonumber
 			  &= \lambda^{-1}_1 \Sigma_{XY}u_1 \sqrt{2 \lambda_1/\pi} \\ \nonumber
 			  &= \sqrt{2 / \lambda_1 \pi} \Sigma_{XY}u_1
\end{align}
Similar calculation yields $\bar{\mu}^X_2 = -\sqrt{2 / \lambda_1 \pi} \Sigma_{XY}u_1$.
The decision rule to classify any observed value of $X$ to $\bar{\mu}^X_1$ is therefore
\[	(\bar{\mu}^X_1)^T X >0	\hspace{0.2in}\text{or} \hspace{0.2in} u^T_1\Sigma_{YX}X>0\] 
Since $u^T_1\Sigma_{YX}X$ is a linear combination of $X$, it also has normal distribution 
\[	\mathcal{N} \left( 0, u^T_1\Sigma_{YX} \Sigma_{XX} \Sigma_{XY} u_1\right)	\]
And $(Y,u^T_1\Sigma_{YX}X)$ also have multivariate normal distribution with mean $\mathbf{0}$ 
and covariance matrix
\[
\begin{bmatrix}
\Sigma_{YY} & \Sigma_{YX}\Sigma_{XY}u_1  \\
u^T_1\Sigma_{YX}\Sigma_{XY} &  u^T_1\Sigma_{YX} \Sigma_{XX} \Sigma_{XY} u_1
\end{bmatrix}
\]
The conditional distribution of $Y|u^T_1\Sigma_{YX}X$ is also multivariate normal with mean 
\[	
\mu_{Y|u^T_1\Sigma_{YX}X } = \Sigma_{YX}\Sigma_{XY}u_1 (u^T_1\Sigma_{YX} \Sigma_{XX} \Sigma_{XY} u_1)^{-1}u^T_1\Sigma_{YX}X	
\]
The $Y$ center for $u^T_1\Sigma_{YX}X>0$ is
\begin{align}
\hat{\mu}^Y_1 &= E(Y|u^T_1\Sigma_{YX}X>0) \nonumber \\ \nonumber
 		      & =  \Sigma_{YX}\Sigma_{XY}u_1 (u^T_1\Sigma_{YX} \Sigma_{XX} \Sigma_{XY} u_1)^{-1} E(u^T_1\Sigma_{YX}X \mid u^T_1\Sigma_{YX}X>0) \\ \nonumber
\end{align}
Note that $u^T_1\Sigma_{YX}X$ has normal distribution $\mathcal{N} \left( 0, u^T_1\Sigma_{YX} \Sigma_{XX} \Sigma_{XY} u_1\right)$, so
\[
E(u^T_1\Sigma_{YX}X \mid u^T_1\Sigma_{YX}X>0) = \sqrt{2/\pi}\cdot\sqrt{u^T_1\Sigma_{YX} \Sigma_{XX} \Sigma_{XY} u_1}
\]
Therefore, we have the $Y$ center for $u^T_1\Sigma_{YX}X>0$ be
\begin{align*}
\hat{\mu}^Y_1 &= \sqrt{2/\pi}\cdot\sqrt{u^T_1\Sigma_{YX} \Sigma_{XX} \Sigma_{XY} u_1} \hspace{0.1in} \Sigma_{YX}\Sigma_{XY}u_1 (u^T_1\Sigma_{YX} \Sigma_{XX} \Sigma_{XY} u_1)^{-1} \\
&=\frac{\sqrt{2/\pi}}{\sqrt{u^T_1\Sigma_{YX} \Sigma_{XX} \Sigma_{XY} u_1}} \Sigma_{YX}\Sigma_{XY}u_1
\end{align*} 
 
Recall that $\bar{\mu}^Y_1 =\sqrt{2 \lambda_1/\pi}u_1$, to judge if $CV(2) > CV(1)$, one only need to compare the distance between  $\hat{\mu}^Y_1$  and  $\bar{\mu}^Y_1$ with distance between  $\hat{\mu}^Y_1$ and grand mean $0$. By variance and bias decomposition of prediction MSE, when variance is the same, only bias influence the MSE. 

After some linear algebra manipulation, we get
$||\hat{\mu}^Y_1 - \bar{\mu}^Y_1||^2 > ||\hat{\mu}^Y_1||^2$ or $CV(2) > CV(1)$ iff
\[
 \frac{\sqrt{\lambda_1}}{2} > \frac{u^T_1\Sigma_{YX}\Sigma_{XY}u_1}{\sqrt{u^T_1\Sigma_{YX} \Sigma_{XX} \Sigma_{XY} u_1}} 
\]
\end{proof}

Above equation gives the condition of when Gabriel CV method would correctly choose $k=1$ over $k=2$. Although the expression is succinct, it's not straight forward to see how the structure of covariance matrix $\Sigma_{XY} \in \mathbb{R}^{(p+q) \times (p+q)}$ affects the performance of Gabriel CV method. Here, assuming covariance matrix has compound symmetric structure where only the matrix dimension $p+q$ and $\rho$ are variables, i.e. $$\Sigma_{XY} = \begin{pmatrix}
1 & \rho & \cdots & \rho \\
\rho & 1 & \cdots & \rho \\
\vdots & \vdots & \cdots & \rho \\
\rho& \rho & \cdots & 1 \\
\end{pmatrix}
$$
we are able to feel what above equation implies for this specific case. Another reason to use the compound symmetric structure covariance matrix is that it's invariant under the permutation of each column vector in the matrix. Given that Gabriel CV method randomly choose $p$ columns as $X$ ($q$ columns as $Y$), compound symmetric structure insures that the covariance matrix $\Sigma_{XY}$ always look the same no matter which $p$ columns selected as $X$.  

If we do $2$ fold cross-validation in the column, i.e. $p=q$, then the result in Proposition $2$ implies that the boundary value of $\rho$ is 
\[
	\rho^* = \frac{1}{p+1}
\]
under the compound symmetric structure given above, where $\Sigma_{XY}$ has $p+q=2p$ dimension. If $\rho < \rho^*$, then Gabriel CV prefers $CV(1)$ over $CV(2)$ and vice versa. It means the boundary value $\rho^*$ depends on the dimension of $\Sigma_{XY}$ linearly. 

If dimension is $p+q = 2$ with $p=q$, then above boundary condition shows the boundary value $\rho^* = 0.5$, while for dimension $p+q = 100$, the  boundary value $\rho^* = \frac{1}{51}$. However, such value also depends on the ratio of $p$ over $q$. Also in $p+q = 100$ dimension, if we pick $p=2$ and $q=98$ (leave majority of the columns for clustering), the the boundary value $\rho^* \in (\frac{1}{6},\frac{1}{7})$. 



\section*{\textbf{APPENDIX}}
\appendix

\section{Technical Lemmas}
\label{app:technical-lemmas}

\begin{lemma}\label{lem:truncated-normal-moments}

If $Z$ is a standard normal random variable, then
\[
  \E(Z \mid a < Z < b)
    = - \frac{\varphi(b) - \varphi(a)}
             {\Phi(b) - \Phi(a)}
\]
and
\[
  \E\{(Z - \delta)^2 \mid a < Z < b\}
    = \delta^2 + 1
    - \frac{  (b - 2 \delta) \varphi(b)
            - (a - 2 \delta) \varphi(a)}
           {\Phi(b) - \Phi(a)}
\]
for all constants $a$, $b$, and $\delta$, where $\varphi(z)$ and $\Phi(z)$ are
the standard normal probability density and cumulative distribution functions.
These expressions are valid for $a = -\infty$ or $b = \infty$ by taking
limits.

\end{lemma}
\begin{proof}
We will derive the expression for the second moment.  Integrate to get
\begin{align*}
  \E[ (Z - \delta)^2 1\{Z < b\}]
    &= \int_{-\infty}^b (z - \delta)^2 \varphi(z) \, dz \\
    &= (\delta^2 + 1) \Phi(b) - (b - 2 \delta) \varphi(b).
\end{align*}
Now,
\[
  \E\{(Z - \delta)^2 \mid a < Z < b\}
    =
    \frac{  \E[ (Z - \delta)^2 1\{Z < b\}]
          - \E[ (Z - \delta)^2 1\{Z < a\}]}
         { \Phi(b) - \Phi(a) }.
\]
\end{proof}

Lemma~\ref{lem:truncated-normal-moments} has some important special cases:
\begin{align*}
  \E\{Z \mid Z > 0\} &= 2 \varphi(0) = \sqrt{2 / \pi}, \\
  \E\{(Z - \delta)^2 \mid Z > 0 \}
    &= \delta^2 + 1 - 4 \delta \varphi(0), \\
  \E\{(Z - \delta)^2 \mid Z < 0 \}
    &= \delta^2 + 1 + 4 \delta \varphi(0).
\end{align*}
\clearpage

\bibliography{references}
\bibliographystyle{apalike}
\end{document}
